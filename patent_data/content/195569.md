# ADAPTIVE PROCESSOR ARRAY

## Claims
Eine selbstanpassende Prozessor Computer Reihenanordnung umfassend eine Mehrzahl identischer Zellen 12 , die in Zeilen und Spalten angeordnet sind, um eine zweidimensionale Matrix zu bilden, wobei jede Zelle eine logische Einrichtung zum Berechnen von Datenwerten und ein erstes Register 70 zum Speichern eines Speicherzustandes B hat, wobei die erste Zeile 14 von Zellen einen parallelen Eingang zu der Reihenanordnung bildet, jede Zelle mit Ausnahme der Endzellen in den Zeilen zwischen der ersten und der letzten Zeile Eingänge hat, von denen jeder einen Datenwert darstellt, der von den zwei Zellen in der vorhergehenden Zeile gekoppelt ist, die diagonal in bezug auf die gegebene Zelle positioniert sind, die Logikeinrichtung in jeder Zelle befähigt ist, einen neuen Datenwert O

## Description
This invention relates to digital parallel processors or processor arrays of the type having a two dimensional matrix of substantially identical interconnected cells adapted to receive signals from other cells in the matrix to perform transformational operations sequentially on a plurality of input signals to produce an output based upon such operations. There are many different examples of processor arrays in the prior art. One of the most fundamentally referred to examples of a processor array is US A 3 106 698, which discloses a two dimensional matrix of identical processing elements or cells having a logic and storage capability. Each cell is connected to a plurality of neighboring cells, and input data or signals may be introduced directly into each cell from an equivalent cell From the above prior art stems a whole realm of parallel processor array structures, many having a principal function of recognition, analysis, digitization or classification of patterns or images. However, these arrays are aimed at precise and accurate results, that is for given inputs there will be definite outputs, i.e., an output for each input, so that different outputs may represent a particular pattern or image event, e.g., an edge. On the other hand, the processor array of this invention has an adaptive behavior, i.e., It is capable of operating in a non linear manner so that various kinds of inputs come to mean certain desired outputs, although the inputs may not necessarily be the same. An array having adaptive behaviour is disclosed in Physical Review Letters, Vol. 52, No. 12, March 1984, pp 1048 51, Adaption and self repair in computing structures , by the present inventors. The article describes a class of computer architectures that compute in a self repairing manner by exploiting the existence of attractors in their phase spaces. Such a mechanism leads to computing structures which are able to learn several inputs reliably and to recognise then even when they are slightly distorted. In one implementation for an array with m rows and n columns of processors, each processor has two integer inputs and one integer output. The local computation rules enhance differences in the data values, with a saturation process which keeps the values within a specified interval. Diagonal lines carry the data down through the array, and horizontal connections allow each processor to adjust its internal state based on its neighbours outputs. For calculating a new output, the differences in the inputs are multiplied by a value stored in an internal memory of each processor. Adaptive behaviour is obtained when the memory values are modified by comparing the output which was previously computed, with the outputs of the adjacent neighbouring processors. The adaptive processor array of this invention is capable of learning certain flexible or variable associations, which learned associations are useful in recognizing classes of inputs, i.e., it is capable of associating certain types of inputs as falling in the same output class or field. The adaptive processor array of this invention comprises a plurality of identical processing cells arranged in parallel orthogonal columns and rows to form a two dimensional matrix. Each of the cells in the array includes logic means and a memory for storing a memory state. The first row of cells in the array forms a parallel input to the array, and the last row of cells in the array forms a parallel output from the array The cells in the array between its first and last rows are individually coupled to two cells in a previous row that are positioned diagonally relative to each such row cell, and are also individually coupled to two cells in a subsequent row that are positioned diagonally relatively to each such row cell. Logic means in each cell computes a new value based upon the two inputs to the cell, which value, based upon whether the cell is programmed to follow a rule of attraction or rule of dissociation, is utilized to move the accumulated value of the cell toward one of two states, one state being a state of dissimilarity, and the other state being a state of similarity. The state of dissimilarity may be referred to herein as a state of dissociation, separation or expansion. The state of similarity may be referred to herein as a state of coalescence or contraction or attraction. The new value towards which the cell is moved is presented as an output to be coupled as an input to two cells in a subsequent row positioned diagonally relative to the output cell. The final parallel output from the last row of the array represents a condition whether the input or inputs presented fall in one or more classes representative of a basin of attraction or a field of inputs. The recognition of inputs being classified in particular classes or fields of input is accomplished by learned association of the inputs initially presented to the array, which learned association is based upon the use of two algorithms which respectively provide a rule of contraction and a rule of expansion utilized in programming of the cells. The present invention will now be described by way of example with reference to the accompanying drawings, in which Reference is made to Figure 1 illustrating a representative example of the adaptive processor array 10 of this invention. The array 10 comprises a plurality of interconnected identical processing cells arranged in parallel Figure 1 represents only a portion of array 10 for purposes of explanation and simplification. The dotted line representations indicate the possible existence of additional rows of cells, after input row 14 of cells and prior to output row 16 of cells as well as additional columns of cells indicated at 18 and 20. Other than the cells in input row 14, each cell has two inputs from two different cells in the previous row, one input being designated the primary input 22 for purposes of explanation, and the other input being designated the secondary input 24. The significance of these input designations will become clearer as the description of the processor progresses. The cells 12 in input row 14 have one input 26, which is a parallel input to array 10 representative, for example, of a numerical physical measurement. Other than the cells in output row 16, each cell has two outputs, which for the sake of simplicity will be also identified as outputs 22 and 24, the same numerals identifying the two inputs from two different cells in a previous cell row. However, the outputs 22 and 24 from a single cell 12 will always be the same or identical, whereas the two inputs 22 and 24 to a cell 12 will not necessarily be the same and will most likely not be the same since these inputs are from two different cell outputs of separate cells in the previous row of cells. It is helpful to view the array 10 from the standpoint of flow path through the cell matrix from the standpoint of the primary input 22. In viewing Figure 1, each of the primary inputs 22 is shown in thicker lines compared with secondary inputs 24. By primary , is meant that the numerical sign on this input is dominant over the numerical sign of the secondary input. Therefore, if the sign on the numerical value on primary input 22 is negative, and the sign on the numerical value on the secondary input 24 is positive, the accepted sign value at the inputted cell will be negative. There is one exception, however. If the numerical value of primary input 22 is zero, then the sign of the secondary input 24 will be the accepted value, which in the example just given would be positive. With this in mind, one can trace the primary inputs 22 downward through array 10 and determine that the primary inputs, as well as the secondary inputs, take a zig zag path through the array, representing, for example, a left input I Reference is now made to both Figures 1 and 2. In Figure 2, there is illustrated one cell 12 from array 10. Cell 12 operates on integral data received from two diagonally positioned neighborhood cells from a previous row of cells Also, supplied to cell 12 are four set conditions wherein, if any of the conditions is true, a certain function is performed. These condition inputs are supplied At each time step, K, each cell 12 receives data values from its diagonal neighbors as integer inputs I The absolute values of inputs I The foregoing description relative to equation 1 involves the state of the integer B remaining the same, i.e., the value of B is frozen. The following explanation deals with rules for changing B other than reset of B to zero. There are two rules, a rule of contraction and a rule of separation. The rule of contraction is also referred to as a rule of attraction or coalescence, and the rule of separation as a rule of expansion or dissociation. In any case, the dynamics of the rules for coalescence and dissociation of possible points or attractors in a basin of attraction or field of response is to operate on the current output, O It will be noted that this rule will drive two inputs toward having the same output by setting the adaptive procedure in local cells to follow this contracting rule. A single application of the rule changes the internal state B of the cell by 1. When a local cell is set for computation following the separating rule, the following algorithm is exploited It will be noted that this rule will drive two inputs toward having separate outputs by setting the adaptive procedure in local cells to follow this separation rule. A single application of the rule changes internal state B of the cell by 1. The following examples will aid in understanding the operation of these two local cell rules wherein the array 10 is to learn associations relative to the state of similarity or dissimilarity. In the case of the contracting rule, assume that the O In the case of the separating rule, assume that O An examination of the contracting rule leads to the interesting phenomenon that its contraction mechanism permits many different inputs into array 10 to be mapped into the same output. In the language of dynamical systems, this corresponds to the appearance of a fixed point in the phase space of the system. The contraction of volumes in phase space makes these fixed points attractive in the sense that perturbations quickly relax back to original values. As shown in Figure 4, inputs 42 are represented in the plane 40 by a multitude of dots. Sets of these inputs can be found or classified in certain fields or basins of attraction. In Figure 4, fields A, B and C are shown. The sets of inputs in each of these fields defines the basin of attraction for a given output, i .e., the inputs are mapped into a specific output which defines the basin of attraction. Thus, the set of inputs 42A in field A will map into output X, the set of inputs 42B in field B will map into output Y and the set of inputs 42C in field C will map into output Z. The contracting rule will force, for example, inputs 42A in basin A all to an output X. Possible inputs outside of field A will be subjected to the separating rule. The basins of attraction can be dynamically modified using the local cell rules in order to include or exclude a particular set of inputs 42. The processes of coalescence and dissociation are achieved by changing the internal state of each computing cell using the adaptive local cell rules, which mimic the global expansion or contraction process desired to be achieved. That such local computation leads to global behavior of this nature is surprising in view of the non linearity of the system. In Figure 5, the global expansion behavior is exemplified for the fields A and B in Figure 4. With the state of contraction or coalescence set to be true, the sets of inputs fed to the array for fields A and B will form a new basin of attraction or field N which will map into a single output W. Tables I and II below show the results of an experiment performed on a ten row by six column processor array 10 obeying the previously explained local cell rules of coalescence and dissociation. These tables illustrate the learning capability of array 10 operating under the local cell rules to appropriately adjust the B value of the cells. This is the learning phase of array operation. In Table I, the results were obtained by sending the inputs through array 10 until a coincidence was obtained between outputs. Table II shows the results obtained wherein inputs which were initially in the same basin or field were subsequently separated. Referring to Table I, the three inputs represent three different samples. For example, they may represent physical parameters such as three samples from a voice pattern. They are all quite different not only in numerical value but also in sign. The integer B in the array cells is set to zero and the contracting rule is set to be true. The original output in Table I shows that the sign of the inputs remained the same after one iteration through the array. It required four iterations through the array to produce the final output shown depicting complete coalescence, i.e., all parallel positive outputs. In Table II, the three inputs shown represent three different samples. Here the samples have all the same sign but are very different in numerical value. The integer B in the array cells is set to zero and the expanding rule is set to be true. The original output in Table II shows that the sign of the inputs remained the same after one iteration through the array. It required three iterations to provide the dissociation in the final output wherein each input was classified to be a different output, one all negative sign, another three sequentially negative and three positive signs and the third sequentially three positive and three negative signs. In the examples of these Tables, the numerical values have not been shown for purposes of illustration. As a continuation of the experiment, the procedure for the above samples of Tables I and II were reversed. The final outputs of Table I were subjected to the expanding rule and it required thirteen iterations to reexpand these coalescent inputs. In the case of the final outputs of Table II, it required five iterations to recontract these dissociated inputs. Having illustrated the adaptive behavior of the local rules of contraction and expansion, an examination of the behavior of inputs nearby a set of inputs in a field can be accomplished during the merging and dissociation of inputs presented to the array. This amounts to determining how the size and shape of the basins of attraction or fields of input change when conditioned reflexes are obtained in an adaptive environment. This is important in establishing to what extent this array is capable of both input generalizations and its complement, differentiation. To proceed with this analysis, the following two techniques were used before and after associating given inputs 1 determining the distribution of sizes of basins of attraction by sending random inputs through the array and counting how many produced each observed output, and 2 determining the size of the basins of attraction by taking each input and measuring the fraction of nearby inputs that are in the same basin of attraction as a function of distance from the original input. Since the set of inputs form an integer lattice, distances between inputs were measured using a street map metric which sets the distance between two points to be the sum of the magnitudes of the differences in each component. Figure 6 illustrates the fraction of three sample inputs that are in each basin of attraction as a function of the distance between the sample input and the learned input in the original cell array with the cell integer B initially set to zero. Figure 7 illustrates the fraction of the same three sample inputs that are in each basin of attraction as a function of the distance between the sample input and the learned input after coalescence. As illustrated in Figures 6 and 7, the process of association produces a broad aggregation of clouds of inputs surrounding the original inputs. This implies that, when associating two specific behavioral responses, the same output can be elicited by operating with inputs which are close to the original ones. Similar results were also observed in the opposite limit of trying to dissociate inputs which originally produced the same outputs. Although the basins of attraction are initially of equal size, after adaptation the fields for the learned inputs grew at the expense of others. Specifically, for the case of an array with 6 columns and 10 rows, there are 26 fields, and each one has 1 64th or 1.6 of the inputs in its basin. After the coalescence experiment described above relative to Figure 7, the field containing the three contracted inputs included almost 4 of the inputs in its basin of attraction. Similarly, in the other experiment in which the three inputs were separated or dissociated, the final basins of attraction contained 4 , 2 and 2 of the inputs, respectively. An interesting consequence of this investigation is the correlation between the ability quickly to associate a set of given inputs and the initial state of the array. Generally, merging two basins of attraction, as illustrated in Figure 5, when starting with all cells having the same state, i.e., a uniform state, was much simpler in taking fewer passes through the array than starting with an array which had already adapted to a particular set of inputs. This became particularly evident in experiments where the initial setup comprised two separate inputs and a uniform array with B set to zero, followed with the application of the contracting rule to associate them together and then followed by the expanding rule to separate them. The time required to separate them was much longer than the time that it took to merge them into the same basin of attraction. In this connection, see the similar results for the inputs of Table I. Figure 3 illustrates an application of array 10. In Figure 3, system 50 comprises two separate cell arrays 10A and 10B of this invention. This architecture provides for pattern recognition of both broad categories and narrow categories relative to input data 26 under investigation. By setting the parameters in the two arrays 10A and 10B to different values, the sizes of basins of attractions in either array can be controlled to be different. For example, array 10A could be set to have large basins with many inputs producing the same output. If the other array 10B is set to produce narrow basins of attractions for the very same inputs, array 10B will distinguish between inputs that array 10A would classify as the same. Thus, by reading output 52 of array 10A first and then reading next output 54 of array 10B, output comparison 56 can provide selection of a broad category or classification e.g., animal Figure 8 is a schematic illustration of the circuitry for an array cell 12. Condition values for cell operation are received The output of logic circuits 90 and 92 on line 94 is a third input to register 70 and will be a new value for the integer B to be stored in register 70 unless line 78 is true, in which case the value B will remain unchanged and the value on line 94 will be ignored. For example when S In operation, assume that the contracting rule is being applied to cell 12, line 66 will be true, enabling contract rule logic 90. Line 64 will not be true, disabling expand rule logic 92. The inputs I In computing O When several sample inputs have been iterated The procedure for initial array learning for the implementation of the expand rule