# PATTERN RECOGNIZING DEVICE WITH PATTERN MATCHING IN SLANT PARALLELOGRAMMIC BLOCKS OF WIDTHS DEPENDENT ON CLASSIFIED REFERENCE PATTERN LENGTHS

## Claims
Einrichtung zum Erkennen eines Eingangsmusters, das Eingangswörter repräsentiert, die entsprechend einer Grammatik kontinuierlich gesprochen wurden, und die eine Eingangsmusterlänge aufweist, welche aus mehreren Rahmen besteht, die aufeinanderfolgend längs einer ersten Zeitachse angeordnet sind, mit

## Description
This invention relates to a device for recognizing by pattern matching an input pattern representative of input words which are substantially continuously spoken or uttered in compliance with a grammar. The pattern matching is carried out between the input pattern and a plurality of reference patterns representative of reference words, respectively, by resorting to a dynamic programming DP technique or algorithm. A device for recognizing an input pattern representative of continuously spoken words, is usually called either a continuous speech recognition device or a connected word recognizing device and has a wide field of application. The continuously spoken words may, for example, be computer programs, sentences in business documents, directions for flight or navigation control, and instructions for various apparatus. It is known in principle that a high reliability is achieved in recognition of an input pattern obtained according to a grammar when the pattern matching is restricted by rules of the grammar. In a relatively simple case, errors are avoided in recognition of an input pattern when a rule is used as a restriction on the number of words of the input pattern in the manner revealed in US A 4,049,9l3. A considerable improvement is introduced to such continuous speech recognition devices by a method and an apparatus disclosed in EP A 0,l62,255 prior art according to Art. 54 3 EPC . The improvement is directed mainly to a system revealed in EP A 82000 The system is for recognizing an input pattern which represents input words continuously spoken according to a grammar. The input pattern has a certain input pattern length. More particularly, the continuously spoken words are represented as the input pattern by a sequence of input pattern feature vectors arranged along a first time axis at consecutive input pattern frame periods, respectively. Each input pattern frame period is herein referred to simply as a frame. It is therefore possible to say that the input pattern length consists of a plurality of frames which are consecutively arranged along the first time axis. In the manner which will later be described a little more in detail, the improved apparatus of the previous Watari patent application is operable according to a slant blockwise DP algorithm wherein each slant parallelogrammic block has a width which is equal to a predetermined number of the frames. The apparatus comprises memory means, concatenating means, matching means, and deciding means. The memory means is for memorizing first through N th reference patterns representative of first through N th reference words, respectively, where N represents a predetermined natural number. An n th one of the reference patterns has an n th reference pattern length where n represents each of one through N. The reference pattern lengths of the respective reference patterns are measured in terms of the frames in the manner which will later become clear. The concatenating means is for concatenating the reference patterns into a plurality of concatenations. Each concatenation consists of selected reference patterns which are selected from the first through the N th reference patterns according to the grammar and are arranged along a second time axis. It is possible to understand without loss of generality that the second time axis is orthogonal to the first time axis. The matching means is for pattern matching the input pattern with the concatenations in slant parallelogrammic blocks to provide dissimilarity measures between the input pattern and the respective concatenations. Each block has a predetermined slope relative to the first time axis and a width and a height which are parallel to the first and the second time axes and equal to a selected number of the frames and to the reference pattern length of each selected reference pattern of each concatenation. The deciding means is for deciding a minimum of the dissimilarity measures to recognize the input pattern as one of the concatenations that is pattern matched to the input pattern to provide the minimum of the dissimilarity measures. More specifically, the selected number should not be longer than a quotient which is equal to the predetermined slope under a minimum of the first through the N th reference pattern lengths. The blocks therefore have widths which are restricted to a narrow width by the minimum reference pattern length. On the other hand, the pattern matching is carried out by accessing various memories a number of times which are reversedly proportional to the widths of the blocks. In other words, the apparatus is operable at a speed which is reversedly proportional to the block width. If only one of the reference pattern lengths is considerably short, the speed becomes slow. The speed must be raised by the use of high speed memory elements as the memories. The apparatus becomes bulky and expensive. It is therefore an object of the present invention to provide a continuous speech recognition device which is highly reliable and is operable at a high speed. It is another object of this invention to provide a continuous speech recognition device of the type described, which is not expensive. Other objects of this invention will become clear as the description proceeds. It is possible on describing this invention to define a continuous speech recognition device as a device for recognizing an input pattern which represents input words continuously spoken according to a grammar and has an input pattern length consisting of a plurality of frames consecutively arranged along a first time axis. The device according to the present invention is as claimed in claim 1 Fig. l is a schematic diagram for use in describing a postpublished prior art algorithm see EP A 0,l62,255 Fig. 2 shows a few lattice points on an i j plane on an enlarged scale Fig. 3 is a schematic diagram for use in describing an algorithm used in a continuous speech recognition device according to the instant invention Fig. 4 is a block diagram of a continuous speech recognition device according to a preferred embodiment of this invention Fig. 5 is a schematic time chart of various signals used in the device depicted in Fig. 4 Fig. 6 is a block diagram of a part of an automaton memory for use in the device shown in Fig. 4 Figs. 7 a through c collectively show a flow chart for use in describing operation of the device illustrated in Fig. 4 Fig 8 is a block diagram of a pattern matching unit for use in the device depicted in Fig. 4 Fig. 9 is a detailed block diagram of a part of the device illustrated in Fig. 4 and Fig. 10 is a block diagram of a decision unit for use in the device shown in Fig. 4. Referring to Figs. l and 2, description will briefly be given at first as regards an algorithm which is used in an apparatus disclosed in the above referenced EP A 0 l62 255 application. This is in order to facilitate an understanding of the present invention. It should be noted that the apparatus is for use in recognizing an input pattern A representative of a string or chain of input words which are substantially continuously spoken in compliance with a grammar. In the manner described also in EP A 82 000 an automaton α is used in describing the grammar for the apparatus. The automaton α is defined by α P, R, S, P₀, F , The reference word set R consists of first, second, ..., n th, ..., and N th reference words where N represents a predetermined natural number. It is possible to identify such reference words by reference word identification numbers assigned to the respective reference words. The reference word identification numbers may be from unity to the predetermined natural number N. In this event, the reference word set R is represented by a symbol n l, 2, ..., N . The input pattern A is supplied to the apparatus as an input pattern signal which can be designated also by the reference letter A. In the manner known in the art, the input pattern signal A is a time sequence of first, second, ..., i th, ..., and I th input pattern feature vectors a₁, a₂, ..., a The input pattern A is alternatively represented by A 0, I where 0 zero represents a zeroth input pattern feature vector which is not used in fact and may be a zero vector. It is possible to understand that the zeroth through the I th input pattern feature vectors are arranged along a first time axis at zeroth through I th input pattern time instants i in the manner depicted in Fig, l. The zeroth and the I th time instants are called an, input pattern start point or time instant and an input pattern end point or, alternatively, an initial and a final point. The input pattern A or A 0, I has an input pattern length which is equal to I when measured in terms of the input pattern frame period. The input pattern length will therefore be designated by the reference letter I. The apparatus keeps the first through the N th reference words as first, second, ..., n th, ..., and N th reference patterns B¹, B², ..., B When measured by a certain unit time, the n th reference pattern B The reference word set R is selected so as to cover the input words used in various strings. In other words, each string is a permutation with repetition of selected reference words which are selected from the reference word set R, namely, from the first through the N th reference words. The selected reference words should be arranged in the permutation according to she grammar. On carrying out comparison with the string, the first through the N th reference words are arranged into various permutations in compliance with the grammar with repetition allowed. Each permutation of the reference words is represented by a concatenation C of first, second, ..., x th, ..., and X th selected patterns B The comparison is carried out by pattern matching of the input pattern A with, in principle, each of such concatenations. It is convenient to understand that the reference pattern feature vectors of the selected pattern are arranged in each concatenation along a second time axis at concatenation time instants j in the manner illustrated in Fig. l, that the second time axis is orthogonal to the first time axis in a time space represented by an i j plane, and that a time interval between two adjacent concatenation time instants is equal to the frame so as to define lattice or grid points i, j on the i j plane in the manner depicted in Fig. 2. The concatenation time instants j are, however, counted from unity up to the reference pattern length, such as J n , for each of the second and the following selected patterns. The first selected pattern alone has the concatenation time instant of zero. During the comparison, each selected pattern of each concatenation is pattern matched to various parts of the input pattern A. Such a part is called a partial or fragmentary pattern and is represented by A u, m , where each of u and m represents one of the zeroth through the I th time instants and where the m th time instant should be later along the first time axis than the u th time instant, The u th and the m th time instants are referred to either as a partial pattern start point and a partial pattern end point or simply as start and end points. The automaton α has a finite number of states which are specified by state identification numbers l, 2, ..., P, ..., and Π where Π represents the finite number. The state set P consists of such states, namely, p l, 2, ..., Π. . During the pattern matching, transition of states occurs in accordance with transition rules. More particularly, the states varies from a start state p to an end state q in response to each selected pattern of each concatenation. Although differently designated for convenience of the description which follows, the end state is one of the states of the state set P. It is unnecessary for each selected pattern that the start and the end states be specified by two consecutive state identification numbers. The end state q for a selected pattern of a concatenation is usually a start state p for another selected pattern that next follows in the concatenation the selected pattern under consideration. It will be said in the following that the start and the end states are had by a selected pattern. For all concatenations, the state transition begins at the initial state P₀ which can be regarded as a zeroth state and will hereafter be represented by a state identification number of 0 zero . Also for all concatenations, the end states are the final states which form the final state set F and belong to the state set P as a subset of the final states. Each transition rule may be written by The pattern matching is carried out by calculating at first a measure or degree representative of either similarity or dissimilarity between the input pattern A and each concatenation. It is convenient for this purpose to understand in the manner described above in conjunction with Fig. l that the input pattern A and the concatenation C are arranged along the first and the second time axes and that the input and the reference pattern feature vectors a Such overall distances are calculated for the respective concatenations by resorting to a dynamic programming DP technique or algorithm. More specifically, a distance recurrence formula is calculated for each start state and each selected pattern which has the start state in question and is used as a current reference pattern, namely, for all pairs p, n , used in the transition rules p, q, n which are included in the state transition table S. The distance recurrence formula may be represented by By using the distance recurrence formula, the pattern matching is carried out in slant or inclined parallelogrammic blocks on the i j plane. The blocks have a common block slope β relative to the first time axis and a common or fundamental block width W in terms of the frame. The block slope is equal to the maximum of path slopes of DP paths exemplified in Fig. 2. In the example being illustrated, the block slope β is equal to two. The block width W is decided according to In practice, the DP technique is carried out by using a minimum recurrence value T m, q and a boundary recurrence value G p, n, j and a first and a second initial condition therefor. The minimum and the boundary recurrence values will become clear as the description proceeds. Under the first initial condition, the minimum recurrence value is equal to zero for T 0, 0 and is infinitely great, namely, sufficiently greater than possible minimum recurrence values, for T m, q other than T 0, 0 . Under the second initial condition, the boundary recurrence value is infinitely great. The distance recurrence formula is iteratively calculated for each pair p, n and then for another pair. For each pair, the iterative calculation proceeds in the manner indicated in Fig. 1 by a double line arrow from a first block of the block identification number of unity to an end block of the block identification number which is equal to I W where it is assumed merely for simplicity of description that the input pattern length I is an integral multiple of the block width W. One of the blocks for which the iterative calculation is in progress, will be called a current block. Each block, if any, will be called a previous block for which the iterative calculation is already carried out. The calculation may be carried out for only those of the blocks which are on both boundaries and within the window known in the art. The iterative calculation is carried out for the current block with a first and a second boundary condition and from a j th start value m The second boundary condition and the j th start and end values are as follows Concurrently with the iterative calculation of the distance recurrence formula for the current block, a pointer recurrence formula is calculated for a pointer h m, j which is known in the art and is alternatively called a path value. The pointer recurrence formula is such that h m When the distance and the pointer recurrence formulae are calculated up to the end block and up to the reference pattern length J n of the current reference pattern, the pattern matching comes to an end between the current reference pattern and a plurality of partial patterns A u, m which have start points u on a first solid line depicted in Fig. l for the start state p and have end points m on a second solid line drawn for the end state q. A pattern end value g m, J n is obtained as the new recurrence value. Attention will now be directed to a pattern end boundary between the current reference pattern and a next subsequently concatenated reference pattern. It will be presumed that a few other reference patterns are already pattern matched as other selected patterns with certain partial patterns and have the end state on the pattern boundary. When such pattern end values are obtained for the current reference pattern and the other selected patterns, minimization is carried out at the pattern end boundary as follows for the pattern end values. When mathematically represented, the minimization at the pattern end boundary is such that if the pattern end value g m, J n for the current reference pattern is less than the minimum recurrence In this manner, the minimum recurrence value T m, q is minimized for a particular reference pattern ñ among the current reference pattern and the other selected patterns. The reference letter p indicates a particular start state which the particular reference pattern has as the start state p. The start point value indicates a particular start point û of a particular partial pattern A û, m that best pattern matches with the particular reference pattern and has the end point at the m th input pattern time instant. When the end state q becomes one of the final states of the final state set F, the pattern matching of the input pattern A comes to an end for concatenations which have that one of the final states in common. In this manner, the input pattern A is pattern matched to the concatenations having the respective final states. At this instant, the pattern end values g m, J n are calculated as concatenation end values for the partial patterns having the end points at the final point I in common and for the selected patterns which have the respective final states as the end states q and stand last in the respective concatenations. The concatenation end values are therefore designated by g I, J n . Minimization at the pattern boundary, which now coincides with pattern ends of the concatenations having a common final state, is carried out as before. After the minimization is carried out for the pattern ends of all concatenations, minima of the concatenation end values give ultimate recurrence value T I, q where the end states q form a final state subset which the final state set F comprises. Among the concatenations, an optimum concatenation is decided as follows starting at a set of initial conditions such that Referring to Fig. 3 anew and to Fig. 2 again, an algorithm will first be described which is for use in a continuous speech recognition device according to this invention. The algorithm is similar to that described above with reference to Figs. l and 2. It is, however, to be noted at first that the first through the N th reference patterns B¹ to B Some of the classes may be empty or void sets. It will, however, be assumed that the k th class comprises at least one reference pattern of the first through the N th reference patterns. The at least one reference pattern has a reference pattern length J n which satisfies In the second place, the slant parallelogrammic blocks are given new variable block widths w. Each variable width w is equal to a selected number of the frames. The selected number is selected in consideration of the block slope β, the reference pattern length of each selected pattern of each concatenation, and the variable natural number k of the class that comprises the selected pattern under consideration. Typically, the variable width w is k times the fundamental or common block width W, namely, equal to kW. Such variable widths w are exemplified in Fig. 3 with an assumption such that the concatenation C comprises first through third selected patterns of successively increasing reference pattern lengths. To speak of the preselected natural number K, it should be noted that the fundamental block width W is decided by the minimum reference pattern length J In the manner which will be described in the following, the distance and the pointer recurrence formulae are calculated in the slant parallelogrammic blocks of the variable widths w. It is convenient to surmise that the block identification numbers b are assigned to the blocks of the fundamental width W as before rather than to the blocks of the variable widths w. In other words, the first time axis is scaled by the fundamental block width W at first rather than by the frame. On iteratively calculating the recurrence formulae, each block identification number b is checked at first whether or not the block identification number is an integral multiple of the variable natural number k. If the block identification number is not the integral multiple, check is secondly carried out whether or not the block identification number is equal to I W. Mathematically, the check is whether or not It should be pointed out in this connection that the common block slope β is predetermined so that each block of the fundamental block width W may have a diagonal which is perpendicular to the first time axis and consequently parallel to the second time axis. Selection of the blocks of the variable block widths w is carried out so that each pair of the blocks may have colinear diagonals. The distance and the pointer recurrence formulae are calculated iteratively for each selected pattern of the k th class and with the zeroth and the j th start and end values given according to Minimization is carried out at the pattern boundary, as above except that the J th end value m It is to be noted that a plurality of blocks of the fundamental width W are processed in parallel in each block of the variable width w. The parallel processing is possible because of the following. First of all, it will be assumed that the minimum reference pattern length J Referring now to Figs. 4 and 5, description will proceed to a continuous speech recognition device according to a preferred embodiment of this invention. A string of input words are substantially continuously spoken to a microphone 2l and thereby converted to an electrical signal of a duration of a speech interval which depends on the number of the input words and durations of the respective input words. Responsive to the electrical signal, an input unit 22 produces a speech interval signal sp. It will be assumed merely for convenience of description that the speech interval signal builds up to a high level from a low level at the beginning of the speech interval and builds down to the low level at the end thereof in the manner depicted in Fig. 5 along a first or top line. Supplied with the speech interval signal, a control unit 23 generates various control signals which are used in controlling other parts of the device as will be described in the following. The input unit 22 furthermore produces the input pattern signal A described heretobefore. The input pattern signal is delivered to an input buffer 24 which is for temporarily storing a part of the input pattern signal. The input pattern signal part will become clear later. The input buffer 24 successively produces some of the input pattern feature vectors a A reference pattern memory 25 is for memorizing the first through the N th reference patterns B¹ to B It is now understood that a combination of the n counter 26 and the selector 28 serves as an activating arrangement responsive to the specified class for activating the reference pattern memory 25 to make the reference pattern memory 25 produce one of the first through the N th reference patterns at a time that the specified class comprises. A combination of the circuit elements 25 through 28 serves as a memory arrangement for memorizing the first through the N th reference patterns with the first through the N th reference patterns classified into the first through the K th classes according to the first through the N th reference pattern lengths. Later in the control unit 23, a concatenation time instant j counter not shown is counted up to generate a reference pattern vector identification signal j which successively specifies the concatenation time instants j for each selected pattern in the known manner from unity up to the reference pattern length J n of the reference pattern specified by the reference pattern identification signal n. In response to the signal j, the reference pattern memory 25 produces the reference pattern feature vectors b Supplied with the input and the reference pattern feature vectors a An automaton memory 38 is for memorizing the automaton α. As soon as the speech interval signal sp builds down, the automaton memory 38 produces the final states in compliance with the final state set F in the manner which is depicted in Fig. 5 along a third line from the top and will later be described more in detail. Responsive to the final states and in cooperation with the first through the fourth table memories 3l to 34, a decision unit 39 carries out the decision of an optimum concatenation n for the string of input words spoken to the microphone 2l. Turning to Fig. 6, the automaton memory 38 comprises first and second memory sections 38l and 382 for the state transition table S like the automaton memory described in EP A 82 000. The first section 38l comprises first through N th memory sectors assigned to the first through the N th reference patterns, respectively, and accessible by the reference pattern identification signal n generated by the control unit 23. Each memory sector is for memorizing at least one start state p of the reference pattern assigned thereto For example, the n th memory sector memorizes a plurality of start states p₁, P₂, ..., p₁, ..., and P In Fig. 6, tie second section 382 comprises first through N th memory blocks allotted to the first through the N th, reference patterns, respectively, and accessible by the reference pattern identification signal n generated by the control unit 23. Each memory block comprises a plurality of memory areas which are assigned to the start states p of the reference pattern allotted to that memory block and are accessible by the start state signal p representative of the start states. Each memory area is for memorizing at least one end state q of the reference pattern for each of the start states. For example, end states q₁ q₂, ..., q, ..., and q Referring afresh to Figs. 7 a through c and more particularly to Fig. 5, operation will be described as regards the device described above with reference to Figs. 4 and 6. When the speech interval signal sp builds up, die control unit 23 generates an initializing pulse as a first set signal SETl depicted in Fig. 5 along a second line from the top. The first set signal initializes the first table memory 3l and the boundary recurrence value table memory 36 as shown in Fig. 7 a at a first step 4l. The control unit 23 comprises a block identification number b counter not shown for use in counting up the block identification number b in the manner depicted in Fig. 5 along a fifth line. At first, the block identification number b is equal to unity as shown in Fig. 7 a at a second step 42. While the block identification number is equal to unity, the class identification number k counter 26 of the control unit 23 counts up the variable natural number k as already described before in conjunction with the sixth line of Fig. 5. At first, the variable natural number k is equal to unity as shown in Fig. 7 a at a third step 43. In the manner indicated at a fourth step 44, the control unit 23 calculates b k. b k , and b k b k and checks whether or not the block identification number b is an integral multiple of the variable natural number k. If NO, the control unit 23 checks at a fifth step 45 whether or not the block identification number b is equal to I W. In the meantime, the variable natural number and the block identification number are counted up in the manner which will later be described. If the block identification number is either an integral multiple of the variable natural number or equal to I W, the reference pattern identification number n counter 27 of the control unit 23 counts up the reference pattern identification number to make the reference pattern identification signal n vary in the manner described in conjunction with the seventh line of Fig. 5. In the manner shown at a sixth step 46, the reference pattern identification number n is equal at first to unity. The control unit 23 meanwhile calculates the zeroth start through end values m By using the reference pattern identification number n and the variable natural number k, the control unit 23 checks at a seventh step 47 whether or not the n th reference pattern B The control unit 23 checks at a ninth step 49 whether or not the combination p, q, n belongs to the state transition rule S. Prior to this instant, the pattern matching is already carried out for the above described current reference patterns having the end states in common at the start state p in the manner which is described hereinabove and will again be described later. The first table memory 3l therefore memorizes the minimum recurrence values T m l, p for the zeroth start through end values which are already calculated at the sixth step 46. In preparation for a tenth step 50, the control unit 23 generates a second set signal SET2 in the manner depicted in Fig. 5 along a ninth line from the top. Turning to Fig. 8 for a short while, the pattern matching unit 29 comprises an absolute value calculator 5l for calculating an absolute value of a vector difference between each input pattern feature vector a In Fig. 8, a recurrence value memory 56 and a pointer value memory 57 are for memorizing the recurrence values g m, j and the pointers h m, j for use as the above described previous recurrence values g m l, j and pointers h m l, j and are coupled to the boundary recurrence and pointer table memories 36 and 37. Through a parallel signal load depicted in Fig. 4 and partly in Fig. 8, the recurrence value memory 56 is connected to the first table memory 3l. Similarly, the pointer value memory 57 is coupled to the control unit 23. At the tenth step 50 Fig. 7 a , the second set signal SET2 sets the minimum recurrence values T m l, p in the recurrence value memory 56 and values of m l in the pointer value memory 57 according to the first and the third boundary conditions, namely, for the zeroth start through end values m In the manner which will presently be described, the control unit 23 Fig. 4 generates the reference pattern vector identification signal j as depicted in Fig. 5 along a tenth line from the top. As soon as the signal j specifies the j th reference pattern feature vector b Subsequently, a first input pattern time instant ml counter not shown of the control unit 23 counts up to generate the afore mentioned first input pattern feature vector identification signal ml which specifies successively the input pattern feature vectors a In Fig. 8, the DP technique is put into practice concurrently with calculation of the distance d m, j in response to the reference pattern vector identification signal j indicative of the j th concatenation time instant and the first input pattern vector identification signal ml indicative of the m th input pattern time instant. If only three previous recurrence values g m l, j are used as described before, the previous recurrence values g m l, j 2 , g m l, j l , and g m l, j are transferred from the recurrence value memory 56 to first through third recurrence value registers 6l, 62, and 63. A comparator 64 decides a minimum of the previous recurrence values g m l, j to produce a selection signal sct which corresponds to the minimum of the previous recurrence values. The comparator 64 furthermore delivers the minimum previous recurrence value g m l, j to an adder 65 which calculates the distance recurrence formula to store the new recurrence value g m, j in the recurrence value memory 56. Three previous pointers h m l, j are likewise transferred to first through third pointer regesters 66, 67, and 68. The selection signal sct selects one of the pointer registers 66 through 68 to calculate the pointer recurrence formula and to provide a new pointer h m, j , which is stored in the pointer value memory 57. Turning back to Figs. 7 a through c , the reference pattern vector identification signal j specifies at an eleventh step 7l of Fig. 7 a the first reference pattern feature vector b The control unit 23 generates at a fifteenth step 75 the first input pattern vector identification signal ml indicative of the j th start value m Subsequently at a ninteenth step 79, the control unit 23 checks whether or not the reference pattern vector identification signal j indicates the concatenation time instant j which is equal to or greater than the n th reference pattern length J n . If NO at the seventeenth step 77, the control unit 23 makes at a twelfth step 80 the first input pattern feature vector identification signal ml indicate a next subsequent input pattern time instant. The steps 76 through 80 are iteratively carried out. If NO at the ninteenth step 79, the control unit 23 makes at a twenty first step 8l the reference pattern vector identification signal j indicate a next following concatenation time instant. The steps 72 through 8l are iteratively carried out. If YES at the ninteenth step 79, the control unit 23 calculates the J th start and end values m Turning temporarily to Fig. 9, minimization is carried out for the pattern end boundary by mainly using the second input pattern vector identification signal m2 and the end states q indicative of the pattern end boundary. The first through the fourth table memories 3l to 34 are already loaded with the minimum recurrence values T m, q , the reference pattern values N m, q , the start state values P m, q , and the start point values U m, q in the manner wihch will presently be described. The recurrence and pointer value memories 56 and 57 are loaded with the pattern end recurrence and pointer values g m, J n and h m, J n . Accessed by the second input pattern vector identification signal m2 and the end state q, the pattern end recurrence value g m, J n and the minimum recurrence value T m, q are supplied to the comparator 35. Only when the former is less than the latter, the comparator 35 delivers a write pulse wp to the table memories 3l through 34. The minimum recurrence value T m, q , the reference pattern identification signal n, the start state signal p, and the pattern end pointer h m, J n are written in the respective table memories 3l to 34. In this manner, the minimum recurrence values T m, q and the like are kept in the table memories 3l and others as values indicative of the selected patterns of various concatenations. Turning back to Figs. 7 a through c again, the comparator 35 Figs. 4 and 9 carries out the minimization in the manner indicated at a twenty seventh step 87. When the pattern end recurrence value g m, J n is less than the minimum recurrence value T m, q , the write pulse wp renews at a twenty eighth step 88 the first through the fourth table memories 3l to 34. At a twenty ninth step 89, the control unit 23 checks whether or not the second input pattern vector identification signal m2 indicates the input pattern time instant which is equal to or greater than the J th end value m If YES at the step 93, the control unit 23 checks at a thirty fifth step 95 whether or not the reference pattern identification number n is equal to or greater than the predetermined natural number N. if NO at the step 49, the check is also carried out. If NO at the step 95, the n counter is counted up at a thirty sixth step 96. The steps 44 through 50 and 7l through 94 are repeated. If YES at the step 93, the control unit 23 checks at a thirty seventh step 97 whether or not the variable natural number k is equal to or greater than the preselected natural number K. If NO at the step 44, the step 97 is also carried out. If NO at the step 97, the k counter is counted up at a thirty eighth step 98. If YES at the step 97, the control unit 23 checks at a thirty ninth step 99 whether or not the block identification number b is equal to or greater than I W. If NO, the b counter is counted up at a fortieth step l00. The steps 43 through 50 and 7l through l00 are repeated. If YES at the step 97, the speech interval signal sp will build down as will be understood from the first and the fifth lines of Fig. 5. Subsequently, the control unit 23 makes the automaton memory 38 Fig. 4 produce the final state set F in the manner depicted along a third line in Fig. 5. It will now be readily possible for one skilled in the art to implement the control unit 23 by using a microprocessor and in consideration of the above described flow chart and to modify the control unit 23 as regards the counters which are not shown in Fig. 4. Turning to Fig. l0, the decision unit 39 may comprise a local controller lll supplied with the final state set F from the automaton memory 38 Fig. 4 . It will be assumed that the controller lll successively specifies first through end final states q₁, ..., and q Responsive to the final states of the final state set F, the first table memory 3l Figs. 4 and l0 produces the ultimate recurrence values T I, q . A combination of a comparator ll2 and a minimum register ll3 finds a minimum of the ultimate recurrence values to decide the above mentioned optimum final state Q. An optimum final state Q register ll4 keeps the optimum final state Q. Thereafter, the controller lll generates a third input pattern time instant identification signal m3 indicative of the final point I. The signal m3 indicates the input pattern time instants from the final point I towards the initial point 0 zero as shown in Fig. 5 along a third line from the top and will shortly be described. On the other hand, the controller lll produces a local end state signal q in the manner which will become clear as the description proceeds. At first, the signal q indicates the optimum final state Q kept in the optimum final state register ll4 as one of the initial conditions for the decision. Responsive to the signals m3 and q indicative of the final point I and the optimum final state Q, the second through the fourth table memories 32 to 34 deliver the optimum reference pattern n, end state q, and start point û to optimum reference pattern n , end state q , and start point û registers ll7, ll8, and ll9. The optimum end state register ll8 thereafter takes over the optimum final state register ll4. The controller lll makes the local end state signal q indicate the optimum end state q and the third input pattern time instant indication signal m3 indicate the optimum start point û. In this manner, the optimum concatenation n is decided by the optimum reference patterns n which are successively produced from the optimum reference pattern register ll7. The local controller lll will now readily be implemented. The controller lll may be included in the control unit 23 Fig. 4 . Referring back to Fig. 7 c , cooperation of the comparator ll2 Fig. l0 and the minimum register ll3 is indicated at a forty first step l2l. Operation of the local controller lll for the initial conditions is shown at a forty second step l22. The iterative indication of the optimum end states q and start points û by the third input pattern time instant indication signal m3 and the local end state signal q is indicated at a forty third step l23. The optimum values n, û and q are decided in the manner indicated at a forty fourth step l24. In the meantime, the local controller lll Fig. 10 checks at a forty fifth step l25 whether or not the optimum start point û coincides with the initial point 0 zero . When the coincidence is found by the check, operation of the continuous speech recognition device comes to an end for the string of input words in question. The device is ready for use in recognizing a different string of input words. While this invention has thus far been described in conjunction with a single preferred embodiment thereof, it will now readily be possible for one skilled in the art to implement various other embodiments of this invention. Above all, the distance recurrence formula may, for example, be When the pattern matching is carried out by the use of a similarity measure, namely, a negative dissimilarity measure, the minimization must be changed to maximization. It should be understood that a device with such a change is an equivalent of the device so far described with reference to Figs. 2 through l0.