# Method of locating processes in a distributed data processing system.

## Abstract
A multi processor, multi tasking virtual machine comprises processes, messages, and contexts. Processes communicate only through messages. Contexts are groups of related processes. The virtual machine is implemented in a dis tributed data processing system comprising a plurality of indi vidual cells coupled by a local area network LAN . Each cell may comprise one or more processes and or contexts. A net work intertace module NIM provides the interface between any individual cell and the LAN. To facilitate message trans mission between processes resident on different cells, each NIM is provided with tables identifying the locations of resident and non resident processes, respectively. Each NIM also provides addressing modes allowing messages to be sent to processes according to the processes names. Cells may be added to or deleted from the LAN without disrupting the LAN operations.

## Claims
CLAIMS 1. A method of locating processes in a distributed data processing system, said system comprising a plurality of individual cells FIG. 11 and at least two processes resident on different ones of said cells, said method comprising the steps of a generating a request by a first process in a first one of said cells to find the location of a second process not resident on said first cell b transmitting said request to a different one of said cells c determining whether said second process is resident on said different cell and d if said second process is resident on said different cell, informing said first cell of the location of said different cell. 2. The method of locating processes as recited in claim 1, and further comprising the step of e if said second process is not resident on said different cell, repeating steps b and c until either the location of said second process is found, in which case the first cell is informed of the location of said different cell, or until all other cells in the system have been queried without finding the location of said second process, in which case the first cell is informed that the second process cannot be found. 3. A method of locating processes in a distributed data processing system, said system comprising a plurality of individual cells FIG. 11 and at least two processes resident on different ones of said cells, said method comprising the steps of a providing in at least one of said cells a table of nonresident processes b generating a request by a first process in said one cell to find the location of a second process identified only by name, said second process not resident on said first cell c transmitting said request to a different one of said cells d determining whether said second process is resident on said different cell and e if said second process is resident on said different cell, storing the location of said different cell in said table. 4. The method of locating processes as recited in claim 3, and further comprising the step of f if said second process is not resident on said different cell, repeating steps c and d until either the location of said second process is found, in which case the location of said different cell is stored in said table, or until all other cells in the system have been queried without finding the location of said second process, in which case the first cell is informed that the second process cannot be found. 5. In a distributed data processing system comprising a plurality of individual cells FIG. 11 , a method of indicating the processes resident within one of said cells, said method comprising the steps of a providing in said one cell a table of resident processes and b making an entry in said table every time a new process is created in said one cell. 6. The method of indicating resident processes as recited in claim 5, and further comprising the step of c deleting an entry in said table every time a process is deleted in said one cell. 7. In a distributed data processing system comprising a plurality of individual cells FIG. 11 , a method of indicating the processes resident within one of said cells, said method comprising the steps of a providing in said one cell a table of resident processes, each entry in said said table comprising a first portion corresponding to the name of a process and a second portion corresponding to the number of processes having the same name which are resident in said one cell b making a new entry in said table every time a process having a unique name is created in said one cell, including indicating the name of said process in said first portion of said table entry and c incrementing said second portion of the appropriate table entry every time a process is created in said one cell whose name is the same as that contained in the first portion of said table entry. 8. The method of indicating resident processes as recited in claim 7, and further comprising the steps of d decrementing said second portion of the appropriate table entry every time a process is deleted in said one cell whose name is the same as that contained in the first portion of said table entry, and e deleting said table entry when said second portion of said table entry reaches a count of zero. 9. A method of communicating between processes in a distributed data processing system, said system comprising a plurality of individual cells FIG. 11 and at least two processes resident on different ones of said cells, said method comprising the steps of a providing in at least a first one of said cells a table of non resident processes, each entry in said table comprising a first portion indicating the name of a process and a second portion indicating the location of a cell where said process is resident b generating a request by a first process in said first cell to transmit a message to a second process identified only by name, said second process not resident on said first cell c looking up the location of said second process in said table in said first cell and finding the location of the cell where said second process is resident and d transmitting said message to the cell containing said second process. 10. The method of communicating between processes as recited in claim 9, and further comprising the step of e if said second process is no longer resident on said different cell, so informing said first cell. 11. A method of communicating between processes in a distributed data processing system, said system comprising a plurality of individual cells and at least three processes resident on different ones of said cells, said method comprising the steps of a generating a request by a first process located in a first one of said cells to transmit a message to a second process and to a third process, said second and third processes being identified only by name, said name being identical for said second and third processes b transmitting said message from said first process to said second process and c transmitting said message from said second process to said third process. 12. The method of communicating between processes recited in claim 11, said method further comprising the steps of d transmitting said message from said third process to said first process and e providing confirmation to said first process in said message that at least one of said second or third processes has received said message. 13. A method of communicating between processes in a distributed data processing system, said system comprising a plurality of individual cells FIG. 11 and a plurality of processes resident on said cells, at least two of said processes being resident on different cells and having the same name, said method comprising the steps ot a generating a request by a first process located in a first one of said plurality of cells to transmit a message to each of said two processes, each of said two processes being identified only by name, said names being identical b transmitting said message from said first process to one of said two processes and c transmitting said message from said one of said two processes to the other of said two processes. 14. The method of communicating between processes recited in claim 13, said method further comprising the steps of d transmitting said message from said other of said two processes back to said first process and e providing confirmation to said first process in said message that at least one of said two processes has received said message. 15. A method of communicating between processes in a distributed data processing system, said system comprising a plurality of individual cells FIG. 11 and a plurality of processes resident on said cells, at least two of said processes being resident on different cells and having the same name, said method comprising the steps of a generating a request by a first process located in a first one of said plurality of cells to transmit a message to at least one but not both of said two processes, said at least one process being identified only by name and b attempting to transmit said message from said first process to one of said two processes. 16. The method of communicating between processes recited in claim 15, said method further comprising the steps of c if said one of said two processes received said message, transmitting said message back to said first process and d providing an indication to said first process in said message that said one of said two processes received said message. 17. The method of communicating between processes recited in claim 15, said method further comprising the step of c if said one process did not receive said message, transmitting said message from said cell containing said one process to the cell containing said second process. 18. The method of communicating between processes recited in claim 15, said method further comprising the step of c if said one process received said message, forwarding said message from said cell containing said one process to the cell containing said second process, in response to a forwarding request by said one process. 19. The method of communicating between processes recited in claim 18, said method further comprising the steps of d transmitting said message back to said first process and e providing an indication to said first process in said message as to whether either of said first and second processes received said message. 20. A method of communicating between processes in a distributed data processing system, said system comprising a plurality of individual cells FIG. 11 and a plurality of processes resident on said cells, at least two of said processes being resident on different cells and having the same name, said method comprising the steps of a generating a request by a first process located in a first one of said plurality of cells to transmit a message to at least one but not both of said two processes, said at least one process being identified only by name b attempting to transmit said message from said first process to one of said two processes and c if said one process received said message, forwarding said message from said cell containing said one process to the cell containing said second process, in response to a forwarding request by said one process.

## Description
NETWORK INTERFACE MODULE WITH MINIMIZED DATA PATHS AND NEW ADDRESSING MODES TECHNICAL FIELD This is invention relates generally to digital data processing, and, in particular, to a network interface module NIM in a distributed data processing system, in which the NIM comprises means for facilitating message transmission between processes resident on different cells in the network, and in which the NIM utilizes modes which facilitate message transmission between processes resident on different cells in the network. BACKGROUND OF THE INVENTION The present invention is implemented in a distributed data processing system that is, two or more data processing systems which are capable of functioning independently but which are so coupled as to send and receive messages to and from one another. A Local Area Network LAN is an example of a distributed data processing system. A typical LAN comprises a number of autonomous data processing cells , each comprising at least a processor and memory.Each cell is capable of conducting data processing operations independently. In addition, each cell is coupled by appropriate means such as a twisted wire pair, coaxial cable, fiber optic cable, etc. to a network of other cells which may be, for example, a loop, star, tree, etc., depending upon the design considerations. As mentioned above, the present invention finds utility in such a distributed data processing system, since there is a significant need in such a system for a relatively great degree of hardware independence. Typical distributed data processing systems comprise a large variety of diverse processors, memories, operator interfaces, printers, and other peripherals.Thus there is an urgent need to provide an operating system for such a distributed data processing system, which operating system will easily accommodate different types of hardware devices without the necessity of writing and or rewriting large portions of such operating system each time a device is added or removed from the system A process , as used within the present invention, is defined as a self contained package of data and executable procedures which operate on that data, comparable to a task in other known systems.Within the present invention a process can be thought of as comparable to a subroutine in terms of size, complexity, and the way it is used.The difference between processes and subroutines is that processes can be created and destroyed dynamically and can execute concurrently with their creator and other subroutines . Within a process, as used in the present invention, the data is totally private and cannot be accessed from the outside, i.e., by other processes. Processes can therefore by used to implement objects , modules , or other higher level data abstractions. Each process executes sequentially. Concurrency is achieved through multiple processes, possibly executing on multiple processors. Every process in the distributed data processing system of the present invention has a unique identifier PID by which it can be referenced. The PID is assigned by the system when the process is created, and it is used by the system to physically locate the process. Every process also has a non unique, symbolic name , which is a variable length string of characters. In general, the name of a process is known system wide. To restrict the scope of names, the present invention utilizes the concept of a context . A context is simply a collection of related processes whose names are not known outside of the context. Contexts partition the name space into smaller, more manageable subsystems. They also hide names, ensuring that processes contained in them do not unintentionally conflict with those in other contexts. A process in one context cannot explicitly communicate with, and does not know about, processes inside other contexts. All interaction across context boundaries must be through a context process , thus providing a degree of security. The context process often acts as a switchboard for incoming messages, rerouting them to the appropriate sub processes in its context. A context process behaves like any other process and additionally has the property that any processes which it creates are known only to itself and to each other. Creation of the process constitutes definition of a new context with the same name as the process. Any process can create context processes. Each new context thus defined is completely contained inside the context in which it was created and therefore is shielded from outside reference This nesting allows the name space to be structured hierarchically to any desired depth. Conceptually, the highest level in the hierarchy is the system itself, which encompasses all contexts. Nesting is used in top down design to break a system into components or layers , where each layer is more detailed than the preceding one. This is analogous to breaking a task down into subroutines, and in fact many applications which are single tasks on known systems may translate to multiple processes in nested contexts. A message is a buffer containing data which tells a process what to do and or supplies it with information it needs to carry out its operation. Each message buffer can have a different length up to 64 kilobytes . By convention, the first field in the message buffer defines the type of message e.g., read , print , status , event , etc. . Messages are queued from one process to another by name or PID.Queuing avoids potential synchronization problems and is used instead of semaphores, monitors, etc. The sender of a message is free to continue after the message is sent. When the receiver attempts to get a message, it will be suspended until one arrives if none are already waiting in its queue. Optionally, the sender can specify that it wants to wait for a reply and is suspended until that specific message arrives. Messages from any other source are not dequeued until after that happens. Within the distributed data processing system described herein, messages are the only way for two processes to exchange data. There is no concept of a global variable . Shared memory areas are not allowed, other than through processes which essentially manage each area by means of messages. Messages are also the only form of dynamic memory that the system handles. A request to allocate memory therefore returns a block of memory which can be used locally by the process but can also be transmitted to another process. Messages provide the mechanism by which hardware transparency is achieved. A process located anywhere in the system may send a message to any other process anywhere else in the system even on another processor and or cell if it knows the process name. This means that processes can be dynamically distributed across the system at any time to gain optimal throughput without changing the processes which reference them. Resolution of destinations is done by searching the process name space. The context nesting level determines the scope of reference when sending messages between processes by name. From a given process, a message may be sent to all processes at its own level i.e., in the same context and optionally to any arbitrary higher level. The contexts are searched from the current context upward until a match is found. All processes with the given name at that level are then sent a copy of the message. A process may also send a message to itself or to its parent the context process without knowing either name explicitly, permitting multiple instances of a process to exist in different contexts, with different names. Sending messages by PID obviates the need for a name search and ignores context boundaries. This is the most efficient method of communicating. There is a significant need to be able to provide within a distributed data processing system the ability to easily add and delete individual cells and or processes in the network s without disrupting network operations. There is also a significant need to be able to communicate easily and quickly between processes which may be resident either in the same cell or in different cells, in a manner which is transparent to the user. BRIEF SUMMARY OF INVENTION Accordingly, it is an object of the present invention to provide a distributed data processing system having an improved network interface. It is also an object of the present invention to provide an improved network interface which facilitates message transmission between processes resident on different cells. It is another object of the present invention to provide an improved network interface which facilitates the addition to and deletion from the network of individual cells without disrupting network operations. It is yet another object of the present invention to provide an improved network interface which provides a message transmission mode that enables all processes with a given name to be addressed. It is still another object of the present invention to provide an improved network interface which provides a message transmission mode that enables the next process with a given name to be addressed. These and other objects are achieved in accordance with a preferred embodiment of the invention by providing a method of communicating between processes in a distributed data processing system, the system comprising a plurality of individual cells and at least two processes resident on different ones of the cells, the method comprising the steps of providing in at least a first one of the cells a table of non resident processes generating a request by a first process in the first cell to transmit a message to a second process identified only by name, the second process not resident on the first cell looking up the location of the second process in the table in the first cell and transmitting the message to the cell containing the second process. BRIEF DESCRIPTION OF THE DRAWINGS The invention is pointed out with particularity in the appended claims. However, other features of the invention will become more apparent and the invention will be best understood by referring to the following detailed description in conjunction with the accompanying drawings in which FIG. I shows a representational illustration of a single network, distributed data processing system incorporating the improved data management system of the present invention. FIG. 2 shows a block diagram illustrating a multiple network, distributed data processing system incorporating the improved data management system of the present invention. FIG. 3 shows an architectural model of a data processing system incorporating the present invention. FIG. 4 shows the relationship between software contexts and processes as they relate to the present invention. FIG. 5 shows how messages may be sent between processes within nested contexts. FIG. 6 shows a conceptual view of the local area network LAN , including several cells, and a representational view of a network interface module NIM . FIG. 7A shows a representation of the NIM s non resident process name cache, and FIG. 7B shows a representation of the NIM s resident process name cache. FIG. 8A shows the format of the discovery message. FIG. 8B shows the format of the discovery response message. FIG. 8C shows the format of the update cache message. OVERVIEW OF COMPUTER SYSTEM With reference to FIG. 1, a distributed computer configuration is shown comprising multiple cells 2 7 nodes loosely coupled by a local area network LAN 1. The number of cells which may be connected to the network is arbitrary and depends upon the user application. Each cell comprises at least a processor and memory, as will be discussed in greater detail with reference to FIG. 2 below. In addition, each cell may also include other units, such as a printer 8, operator display module ODM 9, mass memory module 13, and other I O device 10. With reference now to FIG. 2, a multiple network distributed computer configuration is shown. A first local area network LAN 1 comprises several cells 2,4,and 7. LAN 1 is coupled to a second local area network LAN 2 by means of an Intelligent Communications Module ICM 50. The Intelligent Communications Module provides a link between the LAN and other networks and or remote processors such as programmable controllers . LAN 2 may comprise several cells not shown and may operate under the same LAN protocol as that of the present invention, or it may operate under any of several commercially available protocols, such asEthernet MAP, the Manufacturing Automation Protocol of General MotorsCorp. Systems Network Architecture SNA of International BusinessMachines, Inc. SECS II etc. Each ICM 50 is programmable for carrying out one of the above mentioned specific protocols. In addition, the basic processing module of the cell itself can be used as an intelligent peripheral controller IPC for specialized devices. LAN 1 is additionally coupled to a third local area network LAN 3 via ICM 52. A process controller 55 is also coupled to LAN 1 via ICM 54. A representative cell N 7, FIG. 2 comprises a processor 24 which, in a preferred embodiment, is a Motorola 68010 processor. Each cell further includes a read only memory ROM 28 and a random access memory RAM 26. In addition, each cell includes a Network InterfaceModule NIM 21, which connects the cell to the LAN, and a BusInterface 29, which couples the cell to additional devices within a cell. While a minimal cell is capable of supporting two peripheral devices, such as an Operator Display Module 0DIM 41 and an I O Module 44, additional devices including additional processors, such as processor 27 can be provided within a cell . Other additional devices may comprise, for example, a printer 42, and a mass storage module 43 which supports a hard disk and a back up device floppy disk or streaming tape drive . The Operator Display Module 41 provides a keyboard and screen to enable an operator to input information and receive visual information. While a single cell may comprise all of the above units, in the typical user application individual cells will normally be dedicated to specialized functions. For example, one or more mass storage cells may be set up to function as data base servers. There may also be several operator consoles and at least one cell for generating hard copy printed output. Either these same cells, or separate dedicated cells, may execute particular application programs. The system is particularly designed to provide an integrated solution for factory automation, data acquisition, and other real time applications. As such, it includes a full complement of services, such as a graphical output, windows, menus, icons, dynamic displays, electronic mail, event recording, and file management. Software development features include compilers, a window oriented editor, a debugger, and performance monitoring tools. Local Area Network The local area network, as depicted in either FIG.1 or FIG. 2, ties the entire system together and makes possible the distributed virtual machine model described below. The LAN provides high throughput, guaranteed response, reliability, and low entry cost. TheLAN is also autonomous, in the sense that all system and applications software is unaware of its existence. For example, any NetworkInterface Module e.g. NIM 21, FIG. 2 could be replaced without rewriting any software other than that which directly drives it. The LAN interconnection medium may be twisted pair or coaxial cable. Two channels logically, two distinct networks may be provided for reliability and for increased throughput. The LAN architecture is a logical ring, in which an electronic token is constantly passed from cell to cell at high speed. The current holder of the token may use it to send a frame of data or may pass it on to the next cell in the ring. The NIM only needs to know the logical address and status of its immediately succeeding neighbor.The NIM s responsibility is limited to detecting the failure of that neighbor or the inclusion of a new neighbor. In general, adjustment to failed or newly added cells is automatic. The network interface maps directly into the processor s memory.Data exchange occurs through a dual ported buffer pool which contains a linked list of pending frames . Logical messages, which vary in length, are broken into fixed size frames for transmission and are re assembled by the receiving NIM. Frames are sequence numbered for this purpose. If a frame is not acknowledged within a short period of time, it is retransmitted a number of times before being treated as a failure. As described above with reference to FIG. 2, the LAN may be connected to other LAN s operating under the same LAN protocol via socalled bridgeways , or it may be connected to other types of LAN s via gateways . Software Model The computer operating system of the present invention operates upon processes, messages, and contexts, as such terms are defined herein. Thus this operating system offers the programmer a hardware abstraction, rather than a data or control abstraction. Processes are referenced without regard to their physical location via a small set of message passing primitives. As mentioned earlier, every process has both a unique system generated identifier and a not necessarily unique name assigned by the programmer. The identifier provides quick direct access, while the name has a limited scope and provides symbolic, indirect access. With reference to FIG. 3, an architectural model of the present invention is shown. The bottom, or hardware, layer 63 comprises a number of processors 71 76, as described above. The processors 71 76 may exist physically within one or more cells. The top, or software, layer 60 illustrates a number of processes Pl P10 which send messages ml m6 to each other. The middle layer 61, labelled virtual machine , isolates the hardware from the software, and it allows programs to be written as if they were going to be executed on a single processor.Conversely, programs can be distributed across multiple processors without having been explicitly designed for that purpose. An important purpose of the virtual machine concept hereindisclosed is to provide the applications programmer with a simple, consistent model in which to design his system. This model, as mentioned above, is reduced to several elemental concepts processes, messages, and contexts. As a consequence of this elemental model, hardware peculiarities are made transparent to the user, and changes in hardware configurations have no direct effect on the software. The Virtual Machine A process is a self contained package of data and executable procedures which operate on that data. The data is totally private and cannot be accessed by other processes. There is no concept of shared memory within the present invention. Execution of a process is strictly sequential. Multiple processes execute concurrently and must be scheduled by the operating system. The processes can be re entrant, in which case only one copy of the code is loaded even if multiple instances are active. Every process has a unique process identifier number PID by which it can be referenced. The PID is assigned by the system when the process is created and remains in effect until the process terminates.The PID assignment contains a randomizing factor which guarantees that the PID will not be re used in the near future. The contents of thePID are irrelevant to the programmer but are used by the virtual machine to physically locate the process. A PID may be thought of as a pointer to a process. Every process also has a name which is a variable length string of characters assigned by the programmer. A name need not be unique, and this ambiguity may be used to add new services transparently and to aid in fault tolerance. FIG. 4 illustrates that the system wide name space is partitioned into distinct subsets by means of contexts identified by reference numerals 90 92. A context is simply a collection of related processes whose names are not known outside of the context. Context 90, for example, contains processes A a, a, b, c, d, and e. Context 91 contains processes B, a, b, c, and f. And context 92 contains processes C, a, c, d, and x. One particular process in each context, called the context process , is known both within the context and within the immediately enclosing one referred to as its parent context . In the example illustrated in FIG. 4, processes A C are context processes for contexts 90 92, respectively. The parent context of context 91 is context 90, and the parent context of context 92 is context 91. Conceptually, the context process is located on the boundary of the context and acts as a gate into it. Processes inside context 92 can reference any processes inside contexts 90 and 91 by name. However, processes in context 91 can only access processes in context 92 by going through the context process C.Processes in context 90 can only access processes in context 92 by going through context processes B and C. The function of the context process is to filter incoming messages and either reject them or reroute them to other processes in its context. Contexts may be nested, allowing a hierarchy of abstractions to be constructed. A context must reside completely on one cell. The entire system is treated as an all encompassing context which is always present and which is the highest level in the hierarchy. In essence, contexts define localized protection domains and greatly reduce the chances of unintentional naming conflicts. If appropriate, a process inside one context can be connected to one inside another context by exchanging PID s, once contact has been established through one or the other of the context processes.Most process servers within the present invention function that way.Initial access is by name. Once the desired function such as a window or file is opened , the user process and the service communicate directly via PID s. A message is a variable length buffer limited only by the processor s physical memory size which carries information between processes. A header, inaccessible to the programmer, contains the destination name and the sender s PID. By convention, the first field in a message is a null terminated string which defines the type of message e.g., read , status , etc. Messages are queued to the receiving process when they are sent. Queuing ensures serial access and is used in preference to semaphores, monitors, etc. Messages provide the mechanism by which hardware transparency is achieved. A process located anywhere in the virtual machine can send a message to any other process if it knows its name. Transparency applies with some restrictions across bridgeways i.e., the interfaces between LAN s operating under identical network protocols and, in general, not at all across gateways i.e., the interfaces between LAN s operating under different network protocols due to performance degradation. However, they could so operate, depending upon the required level of performance. Inter Process Communication All inter process communication is via messages. Consequently, most of the virtual machine primitives are concerned with processing messages. The virtual machine kernel primitives are the following ALLOC requests allocation of a message buffer of a given size. FREE requests deallocation of a given message buffer. PUT end a message to a given destination by name or PID . GET wait for and dequeue the next incoming message, optionally from a specific process by PID . FORWARD pass a received message through to another process. CALL send a message, then wait for and dequeue the reply. REPLY send a message to the originator of a given message. ANY MSG returns true if the receive queue is not empty, else returns false optionally, checks if any messages from a specific PID are queued. To further describe the function of the kernel primitives, ALLOC handles all memory allocations. It returns a pointer to a buffer which can be used for local storage within the process or which can be sent to another process via PUT, etc. . ALLOC never fails , but rather waits until enough memory is freed to satisfy the request. The PUT primitive queues a message to another process. The sending process resumes execution as soon as the message is queued. FORWARD is used to quickly reroute a message but maintain information about the original sender whereas PUT always makes the sending process the originator of the message . REPLY sends a message to the originator of a previously received message, rather than by name or PID. CALL essentially implements remote subroutine invocations, causing the caller to suspend until the receiver executes a REPLY.Subsequently, the replied message is dequeued out of sequence, immediately upon arrival, and the caller resumes execution. The emphasis is on concurrency, so that as many processes as possible are executed in parallel. Hence neither PUT nor FORWARD waits for the message to be delivered. Conversely, GETS suspends a process until a message arrives and dequeues it in one operation. The ANY MSG primitive is provided so that a process may determine whether there is anything of interest in the queue before committing itself to a GET. When a message is sent by name, the destination process must be found in the name space. The search path is determined by the nesting of the contexts in which the sending process resides. From a given process, a message can be sent to all processes in its own context or optionally to those in any higher context. Refer to FIG. 5. The contexts are searched from the current one upward until a match is found or until the system context is reached. All processes with the same name in that context are then queued a copy of the message. For example, with reference to FIG. 5, assume that in context 141 process y sends a message to ALL processes by the name x. Process y first searches within its own context 141 but finds no process x. The process y searches within the next higher context 131 its parent context but again finds no process x. Then process y searches within the next higher context 110 and finds a process x, identified by reference numeral 112. Since it is the only process x in context 110, it is the only recipient of the message from process y. If process a in context 131 sends a message to ALL processes by the name x, it first searches within its own context 131 and, finding no processes x there, it then searches within context 110 and finds process x. Assume that process b in context 131 sends a message to ALL processes by the name A. It would find process A 111 in context 110, as well as process A 122 which is the context process for context 121. A process may also send a message to itself or to its context process without knowing either name explicitly. The concept of a logical ring analogous to a LAN allows a message to be sent to the NEXT process in the system with a given name.The message goes to exactly one process in the sender s context, if such a process exists. Otherwise the parent context is searched. The virtual machine guarantees that each NEXT transmission will reach a different process and that eventually a transmission will be sent to the logically first process the one that sent the original message in the ring, completing the loop. In other words, all processes with the same name at the same level can communicate with each other without knowing how many there are or where they are located. The logical ring is essential for distributing services such as a data base. The ordering of processes in the ring is not predictable. For example, regarding FIG. 5, if process a 125 in context 121 sends a message to process a using the NEXT primitive, the search finds a first process a 124 in the same context 121. Process an 124 is marked as having received the message, and then process a 124 sends the message on to the NEXT process a 123 in context 121. Process a 123 is marked as having received the message, and then it sends the message on to the NEXT process a, which is the original sender process a 125 , which knows not to send it further on, since it s been marked as having already received the message. Sending messages directly by PID obviates the need for a name search and ignores context boundaries. This is known as the DIRECT mode of transmission and is the most efficient. For example, process A 111 sends a message in the DIRECT mode to process y in context 141. If a process sends a message in the LOCAL transmission mode, it sends it only to a process having the given name in the sender s own context. In summary, including the DIRECT transmission mode, there are five transmission modes which can be used with the PUT, FORWARD, andCALL primitives ALL to all processes with the given name in the first context which contains that name, starting with the sender s context and searching upwards through all parent contexts. LOCAL to all processes with the given name in the sender s context only. NEXT to the next process with the given name in the same context as the sender, if any otherwise it searches upwards through all parent contexts until the name is found. LEVEL sends to self the sending process or to context the context process corresponding to the sender s context self cannot be used with CALL primitive. DIRECT sent by PID. Messages are usually transmitted by queueing a pointer to the buffer containing the message. A message is only copied when there are multiple destinations or when the destination is on another cell. Further description of the ALL and NEXT transmission modes is found below in the section entitled DETAILED DESCRIPTION OF INVENTION. Operating System The operating system of the present invention consists of a kernel, which implements the primitives described above, plus a set of processes which provide process creation and terminatim, management set time, set alarm, etc. and which perform cell start up and configuration. Drivers for devices are also implemented as processes EESP s , as described above. This allows bath system services and device drivers to be added or replaced eas l. The operating system also supports swapping and paging, although beth are invisible to applications software. Unlike known distributed computer systems, that of the present invention does not use a distinct name server process to resolve names. Name searching is confined to the kernel, which has the advantage of being much faster. A minimal bootstrap program resides permanently in ROM on 9nt cell, e.g. ROM 28 in cell N of FIG. 2. The bootstrap program executes automatically when a cell is powered up and begins by performing basic on board diagnostics. It then attempts to find and start a initial system code module which comprises the entire kernel, and EHSP s ES5P cr the clock, disk if required , and NIM if required . The sod is sought on the first disk drive on the cell, if any. If these ilss. t a disk, and the cell is on the LAN, a message will be sent out requesting the module. Failing that, the required software must be resident 3m. ROM. System services for the clock and for process certion, initialization program, and a minimal file system, are also duilt into the module. The initialization program sets up all of the kennel s internal tables and then calls predefined entry points in each of the preloaded services file management, etc. . The net result is thatEESP s for the attached devices are scheduled to run, and the cell 25 available. In general, there exists a template file describing the initial software and hardware for each cell in the system. The templant defines a set of initial processes usually one per service wki are scheduled immediately after the cell start up. These processes then start up their respective subsystems. A cell configuration service each cell sends configuration messages to each subsystem whan it is being initialized, informing it of the devices it owns.similar messages are sent whenever a new device is added to tbF cell or a device fails or is removed from the cell. Thus there is no well defined meaning for system up or system down as long as any cell is active, the system as a whole may be considered to be up . Cells can be shut down or started up dynamically without affecting other cells on the network. The same principle applies, in a limited sense, to peripherals. Devices which can identify themselves with regard to type, model number, etc. can be added or removed without operator intervention. The operating system cannot maintain a global status of the system, nor does it attempt to centralize control of the entire system. DETAILED DESCRIPTION OF THE INVENTION FIG. 6 shows a conceptual view of the local area network LAN , including several cells, and a representational view of a network interface module NIM . The LAN 404 couples representative cells 1, 2, N . . N 400 402 . Cell 2 401 illustrates further aspects of the invention. A network interface module NIM 406 is illustrated in block diagram form as comprising, among other things, a resident process name cache 440 and a non resident process name cache 441. TheNIM 406 also comprises the kernel, shown generally as 408, and at least one memory store 410, which contains at least one process. In FIG. 6, memory store 410 is shown as containing representative processes B, F, and C. Every other cell coupled to LAN 404 also comprises a NIM likeNIM 406 of cell 2 401 . FIG. 7A shows a representation of the NIM s non resident process name cache 441. The cache is a high speed memory comprising a plurality of addressable memory locations. Each location has at least three fields, as shown by a representative cache word 414, including field 433 containing the process name, field 434 containing the address of the next cell in the LAN where this process is located, and field 435 containing the discovery in process DIP flag. The operation of the NIM s non resident process name cache will be discussed further below. FIG. 7B shows a representation of the NIM s resident process name cache 440. Each cache word comprises at least three fields field 436 containing the process name, field 437 containing the process count, and the transmitted T flag. The operation will be discussed further below. FIG. 8A shows the format of the discovery message 420. The discovery message 420 includes at least a portion 421 containing the process name, and a portion 422 containing the address of the cell originating the message. FIG. 8B shows the format of the discovery response message 424.The discovery response message 424 includes at least a portion 425 containing the process name, and a portion 426 containing the address of the responding cell. FIG. 8C shows the format of the update cache message 428. The update cache message 428 includes at least a portion 429 containing the process name, and a portion 430 containing the address of the cell creating the process. OPERATION OF PREFERRED EMBODIMENT NEXT and ALL Message Transmission Modes As described above, processes may be addressed wherever they reside in the system symbolically by process name. Process names are not required to be unique within the system. Two special message transmission, or addressing, modes are provided to transmit messages between any given proces and any named process NEXT mode and ALL mode. In NEXT mode addressing, the destination process is specified by name and is searched first within the sender s context level ire among all brother processes, regardless if the sender is a context or not . If none is found, the search continues in the parent context, then in the parent s parent context, etc. The first process with the specified name in the first context level where at least one match is found will receive the message. A special case of the NEXT mode addressing is where the destination process name is the same as the sender process name, in which case 1 if the sender is the only process with the specified name in its context, it will also be the receiver of the message, and 2 if there are several processes with the specified name in the same context, each will receive the message. The kernel may implement a logical chain of same named processes within a cell. The NIM provides the link that forms the chain into a logical ring, connecting different cells. If all processes in the chain in one cell have received the message, the message is passed to the NIM for transmission across the LAN to the next process with the same name, wherever it may reside. The NEXT mode message format includes a field designated as the accepted flag. Transmission of a NEXT mode message is considered successful if it returns to the originating cell with its accepted flag set to true. Acceptance means that at least one process with the given name has received the message NEXT mode messages to be transmitted are always passed to the NIM unaccepted . In this case, the message is simply freed. If the message could not be sent or returns without being accepted, the transmission is considered to have failed. Failure of the message to return will result in its retransmission. If the message was sent in the logical ring mode i.e., forwarded by a process to another process with the same name , the message is returned to this process to complete the ring.Otherwise, it is passed to the kernel with transmission error indicated. In ALL mode addressing, the destination process is also specified by name. It is searched for in the sender s context first i.e., among the sender s children if the sender is a context, and among its brothers otherwise . If none is found, the search continues in the parent context, then in the parent s parent context, etc. All processes with the specified name in the first context level where at least one match is found will receive the message except the sender if it happens to have the same name . All ALL mode messages are passed to the NIM for transmission to all, if any, processes residing in other cells on the LAN. The ALL mode message format also includes a field designated as the accepted flag an ALL mode message will be accepted only if one or more processes with the addressed name reside on the originating cell . Transmission of an ALL mode message is considered successful if it returns to the originating cell with its accepted flag set true.In this case, the message is freed. If the message could not be sent, or returns without being accepted, or fails to return at all, the transmission is considered to have failed, and it is passed to the kernel with a transmission error indicated. If the ALL mode message returns to its originating cell with its accepted flag set true, the originating cell is assured that all processes with the addressed name have actually received the message. Non Resident Process Discovery and Caching When the NIM receives a message from the kernel to be transmitted to a named process residing on another cell, it must determine which cell to send it to. Each NIM thus maintains a cache of non resident process names cache 441, FIG. 7A . This cache is a dynamic table consisting of a number of records each of which comprises three field the process name 433, the address 434 of the next cell in which the process resides, and a discovery in progress DIP flag 435. It is important to note that each NIM knows the address of only the next cell containing the named process the next cell may not be the closest cell physically to the NIM . This requires that the cells comprising the system be viewed as forming a logical ring. All messages that are propagated from cell to cell such as discovery messages discussed below are passed in one direction around this logical ring. Another important point is that the non resident process name cache 441 of any given cell contains only the names of the processes to which that cell wishes to transmit messages. The contents of the cache are dynamic, in that process names may be added or deleted. When the NIM receives a message to be transmitted to a named process, it searches its non resident process name cache 441 for the process name. If it s not found, as will be the case initially on power up since the cache is empty , it is necessary to locate or discover the process residency. The process name is registered in a queue in the LAM, and the DIP flag 435 is set true. The message is placed into a holding queue awaiting completion of the discovery. All other messages addressed to the same process while discovery of the process is in progress are also placed into the holding queue. The LAN then builds a discovery message 420, FIG. 8A . The essential elements of the discovery message are the name of the process being discovered and the address of the message originator. The discovery message is sent to the first cell in the logical ring. However, if a logical ring has not yet been established, then the message transmission is terminated, since it is unknown when, if ever, a ring will be formed. Unaccepted messages are returned to the kernel with a transmission error status. Accepted messages are freed.Since the whereabouts of the process has not been determined, the address recorded in the non resident process name cache table 441 is set to the cell s own address. After the discovery message has been successfully sent to the next cell, the message is placed in a timeout queue awaiting either the return of the discovery message or the receipt of a discovery response message 424, FIG. 8B . If a timeout occurs, this indicates that a cell failed before propagating the discovery message, and the discovery message must be retransmitted. If the discovery message returns to the originating cell, then the process does not currently exist in any other cell on the LAN. The discovery message is dequeued from the timeout queue on receipt of either the returning discovery message or the corresponding discovery response message. When a NIM receives a discovery message off the LAN, it searches its resident process name cache for the name in the message. If it finds it, the discovery message is then transformed into a discovery response message, which identifies the resonding cell and is sent to the message originator. If, however, the process doesn t exist in this cell, the discovery message is sent to the next cell in the logical ring. The message is propagated in this way until either it is received by a cell with the named process or it returns to the originating cell. If the discovery message returns to the originating cell indicating the named process doesn t exist externally, an invalid cell address indicating the process doesn t exist externally is placed in the process record in the non resident process name cache table.Since discovery has been completed, the DIP flag is set to false. All messages addressed to the named process are dequeued from the holding queue. Messages that were originated by another cell are returned to that cell. Messages which were originated by this cell are either returned to the kernel with a transmission error indicated or are simply freed. If a discovery response message is received by a NIM, the address of the responding cell is recorded in the non resident process name cache table, and the DIP flag is set to false, since the discovery has been completed. All messages addressed to the discovered process are dequeued from the holding queue and sent to the cell that responded.These messages are placed in a timeout queue upon successful transmission to the cell to await their return. When a NIM receives an ALL mode or NEXT mode message addressed to it off the LAN, it searches its resident process name cache 440, FIG.7B for the name of the addressed process. If it s not found, the message is returned to the sender. This happens, for example, if the process has been deleted. On receipt of the rejected message the sending NIM must rediscover the address of the next cell containing the process and update its non resident process name cache. If the name is found in the resident process name cache, the message is passed to the addressed process through the kernel. If the message has not already been accepted the NIM must wait for the kernel to indicate whether the message was accepted. This is required since there exists a time window between the checking of the resident process name cache and the kernel being handed the message during which the process may be deleted. NEXT mode messages are always passed to the NIM with the accepted flag set false. So the NIM at the receiving end must always wait for the kernel to indicate its acceptance. On receipt of this response from the kernel, the message is returned to the originator.If the message was accepted, the originator will dequeue it from the timeout queue and consider the transmission successful. If, however, the message was not accepted due to the process having been deleted, the originating NIM will attempt to rediscover the process location. An ALL mode message may be immediately propagated to the next cell containing the named process only if the message has already been accepted. If the message has not been accepted, its propagation must be delayed until the kernel has indicated whether it was accepted. Both NEXT mode and ALL mode messages eventually return to the originating cell. If a cell fails before propagating the message, the originating cell will time out. NEXT mode messages will be transmitted. An ALL mode message, however, cannot be retransmitted, since it is not known which cells received the message and which did not. Depending upon whether the message was accepted, it is either freed or returned to the kernel with a transmission error status. Process Creation and Deletion Whevever a process is created or deleted, the kernel informs theNIM. The NIM maintains its own resident process name cache, which is a dynamic table consisting of one record per resident process name. Each record has three fields the name of the process, a process count, and a transmitted flag refer to FIG. 7B . Whenever a process is created, the cache is searched for the name. If it s not found, a record is entered for the process with the count set to one and the transmitted flag set to false. If the name is already in the resident process name cache, the process count is simply incremented. When a process is deleted, the count in the record corresponding to the process is decremented. When it reaches a value of zero, the named process no longer exists in the cell, and the record is removed from the resident process name cache. Whenever a new process is created, an update cache message refer to FIG. 8C is built and transmitted to the next cell in the logical ring. The transmitted flag is set to true. Once transmitted, the message is placed in a timeout queue awaiting its return. The update cache message contains the name of the created process and the address of the cell in which it was created. On receipt of such an update cache message, a NIM searches its non resident process name cache for the process. If found, the non resident process name cache is updated if the cell containing the new process is closer than the current cell listed which contains this process. The update cache message is passed from cell to cell around the logical ring until it arrives at the originating cell. Failure of the cell to propagate the message will cause the originating cell to time out and retransmit the message. No special action is taken on process deletion. A NIM will discover that a process has been deleted from a cell when an ALL mode or NEXT mode message sent to that cell is rejected and returned. The sending cell will then attempt to rediscover the process and update its non resident process name cache. DESCRIPTION OF PROGRAM LISTINGS Program Listings A E contain a C language implementation of the concepts relating to message transmission among processes in a data processing system incorporating the present invention. The implementation, in particular, of the innovative features of the present invention as set forth and described above will now be described. Program Listing A Program Listing A maintains the non resident process name cache.All NEXT and AM. mode messages are passed through this process.Lines 158 186 This code is executed if the message was originated by another cell. The first section handles messages being returned by the kernel and restores the message identifier. The second section saves the message as received off the LAN if not accepted so that the identifier can be restored. The last section returns NEXT mode messages to their origin.Lines 188 192 This code handles the case where the named process is not found in the non resident process name cache. A record is acquired, initialized, and placed into the cache. The message is then placed in the holding queue and a discovery message built and transmitted in Program Listing B.Lines 194 208 This code handles the case where the named process already exists in the cache. If discovery is already in progress, the message is just placed in the holding queue. If discovery is still required, celladdr self , then the message is placed in the holding queue, and a discovery message is issued. If the address in the cache is zero, the process does not exist externally. Otherwise the message is sent to the cell.Lines 215 217 This code handles the unsuccessful transmission of an ALL or NEXT mode message. Rediscovery will be attempted if possible.Lines 219 230 This code handles the unsuccessful transmission of a discovery message. If originated by this cell, it will be retransmitted if possible, or the messages in the holding queue will be dequeued. If it was originated by another cell, it will either be discarded or retransmitted.Lines 232 239 This code handles the unsuccessful transmission of an update cache message originated by another cell. It will be retransmitted if possible or discarded.Lines 248 274 This code handles received discovery and discovery response messages. The appropriate non resident process name cache record is updated. The messages in the holding queue are then dequeued. If the addressed process exists externally, the messages are sent to the cell. Otherwise the messages are returned to the originating cell or returned to the kernel.Lines 277 286 This code deals with the receipt of update cache messages. The cache is searched for the name and updated if required.The message is then sent to the next cell. Lines 289 291 This code handles the receipt of a reject ALL orNEXT mode message. The named process is rediscovered to allow retransmission of the message.Lines 300 313 This code is entered when an ALL or NEXT mode message originated by this cell fails to return, and a time out occurs.If possible, a NEXT mode message will be resent. If not, or if an ALL mode message timed out, the message is returned to the kernel.Line 316 This code retransmits a discovery message originated by this cell which failed to return in time. If not possible, the messages in the holding queue are dequeued. Program Listing B Program Listing B is called by Program Listing A when discovery is required.Lines 64 79 This code is executed if another cell exists on theLAN. The ALL or NEXT mode message is placed in the holding queue, and a discovery message is built and sent to the next known cell.Lines 82 89 This code is executed if no other cells are up.The cache record is updated to indicate that discovery is still required. If the message was originated by this cell, it is returned to the kernel. Otherwise, it is discarded. Program Listing C Program Listing C is called by Program Listing A on various error conditions.Lines 67 68 If another cell exists, the discovery message is transmitted to the next cell.Lines 70 84 If there are no other cells, the cache record is updated to indicate that discovery is still required. All messages addressed to the affected process are dequeued from the holding queue and returned to the kernel. Program Listing D Program Listing D is called whenever any message is received off the LAN.Lines 84 90 If the addressed process name is not found in the resident process name cache, the message is rejected and returned to the sender.Lines 93 104 If the addressed process exists in this cell, a copy of the message is made and passed to Program Listing A. The original message is passed to the destination process through the kernel. Program Listing E Program Listing E is informed by the kernel of all process creations and deletions.Lines 130 162 This code is executed periodically while signalon is true. If another cell exists on the LAN, it scans through the resident process name cache and transmits update cache messages for any process names that have not yet been sent.Lines 201 241 This code is executed whenever a process is created. If the process is a new one, a resident process name cache record is obtained and initialized. If another cell exists on the LAN, an update cache message for the new process is sent to the next cell.Finally, the process count is incremented.Lines 244 263 These lines handle update cache message time outs.If the affected process name still exists and there is a cell to send to, then the update cache message is retransmitted. If there are no other cells, then the record is marked as not being sent.Lines 267 273 This code is executed whenever a process is deleted.It simply decrements the process count and, if it reaches zero, frees the record. It will be apparent to those skilled in the art that the herein disclosed invention may be modified in numerous ways and may assume many embodiments other than the preferred form specifically set out and described above. Accordingly, it is intended by the appended claims to cover all modifications of the invention which fall within the true spirit and scope of the invention. What is claimed is PROGRAM LISTING A 10 MODULE NAME 11 12 SUBSYSTEM NAME NIM 13 14 DATE TIME CREATION XEX XU 15 16 DATE TIME OF COMPILATION XD XI 17 18 VERSION 19 20 PROGRAMMER Bernie Weisehner 21 22 DESCRIPTION 23 Context name discovery process.24 25 26 PARAMETERS PASSED None 27 28 29 PARAMETERS RETURNED None 30 31 32 3 staticd cher Sreid X2 XM XI 34 35 36 37 num include local cx.io 38 num include kerntypec.h 39 include nimkern.h 40 finclude nimgen.h 41 num include locan nim nimif.h 42 43 num define self hwv celLaddr 44 num define RECPERSEG 10 45 46 47 int trg ifdisp og 48 int trg ifdisp ig 49 50 51 52 num ifdef CXE 53 int ifv offset record offset to name field 54 long nwov pid pid of nwop 55 long hwchamgr pid pid of hwchamgr 56 long hwchbmgr pid pid of hwchbmgr 57 Long hwchamgr pid pid of hwchcmgr 58 int hwv celladdr this cell s physical address 59 num else 60 extern int ifv offset record offset to name field 61 extern tong nwov pid pid of nwop 62 extern long hwchamgr pid pid of hwchamgr 63 extern long hwchbmgr pid pid of hwchbmgr 64 extern Long hwchamgr pid, pid of hwchcmgr 65 extern int hwv celladdr this cell s physical address 66 endif 67 68 69 70 IFDISSD MANQUE ifdisv nqhead context name queue head pointer 71 IFDISSD MANQUE ifdisv notail context name queue tail pointer 72 IPCSD TXRXMSG ifdisv hldhead holding queue head pointer 73 IPCED TXEXMEG ifdisv hldteil holding queue tail pointer 74 75 76 77 PROCESS ifdisp 78 79 extern IFDISSD NAMQUE genr nam 80 extern 1FD1SSD MANQUE penr gor 81 extern void genr alk 82 extern void genrcopy 83 extern void genr prnt 84 extern void genr ik 85 extern void genr ulk 86 extern void genr free 87 extern bool nwor beynd 88 extern void ifdisr txd 89 extern void ifdisrfail 90 extern vo itcomr txs 91 extern unsigned int ifor nc 92 93 IFDISSD NAMQUE ifdisv avnead avatlable queue head pointer 94 IFDISSD NAMQUE ifdisv avtail avaitable queue tail pointer 95 IPCSD TXPXMSG ifdisv idtail msg id queue head pointer 96 IPCSD TXRXMSG ifdisv idtail msg id queue tail pointer 97 IPCSD TXRXMSG ifdisv midhead msg id holding queue head pointer 98 IPCSD TXRXMSG ifdisv midtail msg id holding queue tail pointer 99 register IPCSD TXRXMSG msgptr pointer to txrxmsg IPC message 100 register IPCSD TXRXMSG msgptr2 pointer to txrxmsg IPC message 101 register IPCSD TXRXMSG msgptr3 pointer to txrxmsg IPC message 102 register IFDISSD NAMQUE recptr pointer into context name queue 103 int ipcmsg pointer to received IPC message 104 char namptr pointer to context name string 105 int i loop counter 106 unsigned int destin P destination cell address 107 108 ifdisv hldhead ifdisv hldtail 0 109 ifdisv avnead ifdisv avtail ifdisv nohead ifdisv ngtait 0 110 ifdisv idhead ifdisv idtail ifdisv midhead ifdisv midtail 0 111 num ifdef DLDKERN 112 while ipcmsg Get NULL, NULL int INIT 113 num else 114 while ipomsg int Get NULL, NULL, NULL int INIT 115 num endif 116 Free ipcmsg 117 num ifdet CXE 118 ifv offset IPSCD INIT ipcmsg ifv offset 119 nwov pid IPCSD INIT ipcmsg pid 120 hwchamgr pid IPCSD INIT ipcmsg hwchamgr pid 121 hwchbmgr pid IPCSD INIT ipcmsg hwchbmgr pid 122 hwchcmgr pid IPCSD INIT ipcmsg hwchcmgr pid 123 hwv celladdr IPCSD INIT ipcmsg hwv celladdr 124 num endif 125 Free ipcmsg 126 genrprntC ifdisp initialized n 127 128 FOREVER 129 trg ifdisp eg 1 130 num ifdef OLDKERN 131 ipcmsg Get NULL, NULL 132 num clse 133 ipcmsg int Get NULL, NULL, NULL 134 num endit 135 trg ifdisp lg ipcmsg 136 switch ipcmsg 137 138 case INT start of code for INT 139 printf nifdisv nghead x, ifdisv ngtail x n , ifdeisv nqhead, ifdisv nqtail 140 if ifdisv nqhead 1 0 141 printf discinprog cell context name n 142 for recptr ifdisv nqhead recptr 1 0 recptr IFDISSD NAMQUE recptr inks.next 143 printf 1d 2d s n , recptr discinprog, recptr celladdr, recptr name 144 145 printf ifdisv htahead x, ifdisv hldtail x n , ifdisv hldhead, ifdisv hldtail 146 if ifdisv hlahead 1 0 147 printf txmode context name n 148 for msgotr ifdisv hlahead msgptr 1 0 msgptr IPCSD TXRXMSG msgptr Links.next 149 printf 1d s n , MSG msgptr msgaddr mode, msgptr dest spec.p name 150 151 printf ifdisv iahead x, ifdisv idtait x n , ifdisv idhead, ifdisv idtait 152 printf ifdisv midnead x, ifdisv midtail x n , ifdisv midnead, ifdisv midtail 153 Free ipcmsg 154 break end of code for INT 155 156 case TXMSG 157 msgptr IPCSD TXRXMSG Ipcmsg 158 if msgptr msgorg self C 159 if msgptr msgid 0 160 if msgptr2 ifdisv idhead 0 161 gen lk fdisv midhead, fdisv midtail, msgptr 162 msootr 0 163 3 else C 164 msgptr msgid msgdtr2 msgid 165 msgptr msgorg msgptr2 msgorg 166 genr ulk fdisv idhead fdisv idtail, msgptr2 167 genrfree msgptr2 168 169 else 170 if MSGf msgptr msgaddr flags ACCEPTED 171 if msgptr2 ifdisv midhead 0 C 172 genr lk fdisv idhead fdisv idtail msd te . 173 msgptr 0 174 3 else C 175 msgptr2 msgid msgptr msgid 176 msbdtr2 ms org ms tr ms 177 genr ulk fdisv midhead, fdisvmidtail, msgptr2 178 genr free msgptr 179 msbdtr msbdtr2 180 3 181 if MSG msgptr msgaddr mode unsigned char NEXT amp MSG msgptr msgaddr . f lags ACCEPTED C 182 msgptr msgsrcdst msgptr msgorg 183 Forward DIRECT, nwov pid, msgptr 184 msgptr 0 185 186 187 if msgptr 0 188 if recptr genrnam fdisv nqhead, msgptr dest spec.p name, ifvoffset 0 C 189 recptr genr gar fdisv avhead, fdisv avtail, sizeof IFDISSD NAMQUE , RECPERSEG 190 strncpy recptr name, msgptr dest spec.p name, MAXNAM 191 genr alk fdisvnqhead, fdisv nqtail, recptr, ifvoffset 192 ifdis txd msgptr, recptr 193 else 194 if recptr discinprog 195 genr lk fdisv hldhead, fdisv hldtail, msgptr 196 eLse 197 if recptr celladdr 0 198 if recptr celladdr self 199 ifdisr txd msgptr, recptr 200 else C 201 msgdtr msgsrcdst recptr celladdr 202 Forward DIRECT, nwov pid, msgptr 203 204 else C 205 msgptr ipcmsgtype int TXSTAT 206 msgptr txrx.txstat.status FAILURE 207 ifcpmrtxs sgptr 208 209 break 210 211 case TXSTAT 212 msgptr CIPCSDTXRXMSG ipcmsg 213 switch msgptr msgtype C 214 case MSG TYP CPMMSG 215 recptr genr nam fdisv nghead, msgptr dest spec.p name, ifv offset .216 ifdisr txd msgptr, recptr 217 break 218 case MSG TYP DIS 219 if msgptr msgorg self 220 ifdisrfail msgptr 221 else 222 msgptr msgsrcdst ifor ne 223 if nwor neynd msgptr msgsrcdst, msgptr msgorg I msgptr msgsrcdst self 224 genrtree msgptr 225 else C 226 msgotr ipcmsgtype int TXMSG 227 Forward DIRECT, nwov pid, msgptr 228 229 3 230 break 231 case MSG TYO UPD 232 msgptr msgsrcdst iforncC 233 if nwor beynd msgptr msgsrcdst, msgptr msgorg II msgptr. msgsrcdst self 234 genrT.ree msgptr 235 else C 236 msgptr ipcmsgtype int TXMSG 237 Forward DIRECT, nwov pid, msgptr 238 3 239 break 240 241 break 242 243 case RXMSG 244 msgptr IPCSDTXRXMSG ipcmsg 245 switch msgptr msgtype C 246 case MSG TYP DISRSP 247 case MSG TYP DIS 248 namptr char msgptr 1 249 recptr genr nam fdisv nghead, nm otr, ifvoffset 250 recptr celladdr msgptr msgtype MSG TYP DISRSP msgptr msgsrcdst 251 recptr discinprog FALSE 252 msgptr2 ifdisv hldhead 253 while msgptr2 0 254 if strncmo namptr, msgptr2 dest spec p name, MAXHAM O 255 msgptr3 CIPCSD TXRXMSG msgdtr2 links next 256 genrulk fdisvhldnead, ifdisv hidtail, .sgptr2 257 if retptr celladdr 1 0 258 msgptr2 msgsrcdst recptr celladdr 259 msgptr2 ipcmsgtype int TXMSG 260 Forward DIRECT, nwov pid, sgptr2 261 else 262 if msgntr2 msgorg self C 263 msgptr2 msgsrcdst msgptr2 msgorg 264 msgptr2 ipcmsgtype int TXMSG 265 Forward DIRECT, now pid, msgptr2 266 3 else C 267 msgptr2 ipcmsgtype int TXSTAT 268 msgptr2 txrx txstat status FAILURE 269 ifcpmr txs msgptr2 270 271 msgptr2 msgptr3 272 3 else 273 msgptr2 IPCSD TXRXMSG msgptr2 links next 274 genrfree msgptr 275 break 276 case MSG TYP UPD 277 if recptr genr nam fdisv nghead, msgptr 1, ifv offset 1 0 278 if lrecptr discinprog 279 if Inwor beynd msgptr msgorg, recptr celladdr 280 recptr celladdr msgptr msgorg 281 msgptr ipcmsgtype int TXMSG 282 if msgptr msgsrcdst ifor nc 1 self C 283 msgptr ipcmsgtype int TXMSG 284 Forward DIRECT, nwov pid, msgptr 285 3 else 286 genr free msgptr 287 break 288 case MSG TYP CPMREJ 289 msgptr msgtype MSG TYP CPMMSG 290 recptr genr nam fdisv nghead, msgptr dest spec p name, ifv offset 291 ifdisr txd msgptr, recptr .292 break 293 294 break 295 296 case IFTO 297 msgptr CIPCSDTXRXMSG ipcmsg 298 switch msgptr msgtype C 299 case MSG TYP CPMMSG 300 if MSG msgptr msgaddr mode char NEXT 301 recptr genr nam ifdisv nghead, msgptr dest spec p name, ifv offset 302 if recptr celladdr 0 u recptr telladdr self C 303 msgptr ipcmsgtype int TXMSG 304 esgptr msgsrcdst recptr cell ddr 305 Forward DIRECT, nwovpid, msgptr 306 msgptr 0 307 308 309 if msgntr 1 O C 310 msgptr ipcmsgtype int TXSTAT 311 msgptr txrx txstat status FAILURE 3i2 ifccartxs ssgptr 313 3 314 break 315 case MSG TYP DIS 316 ifdisr fail msgptr 317 break .318 3 319 break 320 321 default 322 Forward NEXT, error , ipcmsg 323 break 324 3 325 3 326 PROGRAM LISTING B 10 HODULE NAME 11 12 SUBSYSTEM NAME NIM 13 14 DATE TIME OF CREATION E XUX 15 16 DATE TIME OF COMPILATION D T 17 18 VERSION 19 20 PROGRAMMER Bernie Weisahear 21 22 DESCRIPTION 23 Function to send a discovery message. 24 25 26 PARAMETERS PASSED 27 msgotr addr of message to be transmitted to named context 28 recptr addr of corresponding name queue record 29 30 31 PARAMETERS RETURNED None 32 33 34 35 static char SRcld . Z M I 36 37 38 39 num include local cx h 40 aincluoe kerntypes.h 41 num include nimkern.h 42 num include nimgen.h 43 num include local nim nimif.h 44 45 46 47 void ifdisrtxd msgptr, recptr 48 49 register IPCSDTXRXMSG xmsgotr addr of msg to be sent to named context 50 register IFDISSD NAMQUE recptr addr of corresponding name queue record 51 52 C 53 extern void gen copy 54 extern void genrlkc 55 extern void pent frrr 56 extern unsigned in ifor nc 57 extern void ifcpmr txs 58 extern unsigned in hwv celladdr 59 extern IPCSD TXRXMSG ifdisv hldnead, ifdisv hldtail 60 extern Long nowv pid 61 register unsigned int destin 62 register IPCSD TXRXMSG msgptr2 63 64 if destin ifor nc hwv cettaddr C 65 recptr discinprog TRUE 66 genr LK fdisv hldnead, fdisv hldtail, msgptr 67 msgptr2 IPCSD TXRXMSG Alloc sizeof IPCSD TXRXMSG MAXNAM 68 genr copy msgptr dest spec p name, msgptr2 1, MAXNAM 69 msgptr2 ipcmsgtype int TXMSG 70 msgdtr2 msgaddr unsigned char msgptr2 1 71 msgdtr2 msgten MAXNAM 72 msgdtr2 msgid 0 73 msgptr2 msgtype MSG TYP DIS 74 msgptr2 msgdri MAXPR1 75 msgDtr2 msporo hwv cellsdar 76 msgdtr2 mgsrcast destin 77 msgptr2 singte TRUE 77 num ifdef OLDKERN 79 Put DIRECT, nwov pid, msgdtr2, MAXPRI 80 num else 81 Put OIRECT, nwov pid, msgptr2 82 wendif 83 else C 84 recdtr discinprog FALSE 85 recptr celladdr hwv celladdr 86 if msgdtr msgdrg nwvcelLaddr C 87 msg tr iocmsgtype int TXSTAT 88 msgptr txrx txstat.status FAILURE 89 ifcpmr txs msgptr 90 3 else 91 genr free msgptr 92 93 PROGRAM LISTING C 10 MODULE NAME 11 12 SUBSYSTEM NAME NIM 13 14 DATE amp TIME OF CREATION E 15 16 DATE TIME OF COMPIL ION D 17 18 VERSION R 1 19 20 PROGRAMHER Bernie Weisshaar 21 in DESCRIPTION 23 function to send a discovery message.24 25 26 PARAMETERS PASSED 27 asgptr addr of DIS message that failed 28 2g 3D PARAMETERS RETURNED None 31 32 33 1 34 sratic char S ld Z N I 35 36 37 38 num include local cx.h 39 num include kerntypes.h 40 num include nimkern.h 41 num include nimgen.h 42 num include local nim nimif.h 43 44 45 46 void ifdisr fail msgptr 47 48 register IPCSD TXRXMSG msgptr sddr of DIS messagt that falled 49 50 51 extern void genr copy 52 extern void genr lk 53 extern void genr free 54 extern IFDISSD NAMQUE genr ne 55 extern unsigned int ifor nct 56 extern void ifcomt txs 57 astern unsigned int hw telladdr 58 extern IPCSD TXRXMSG ifdisv bidnead, ifdisv hidtail 59 extern IFDISSD HAMQUE ifdisv nghead 60 extern int ifv offset 61 extern long nwov pid 62 63 register IFDISSD NAMQUE recptr 64 register IPCSD TXRXMSG msgptr2, msgptr3 65 register char namptr 66 67 if msgptr msgsrcdst ifor nc 1 hwv celladdr 68 msgdtr pcmsgtype int TXMSG 69 Forward DIRECT, nwov pid. msgptr 70 else C 71 namptr char msgptr 1 72 recptr genr nam num ifdisv ngnead, namptr. ifv offset 73 recptr discinorog FALSE 74 recptr delladdr hwv celladdr 75 msgptr2 ifdisv hldnead 76 while msgptr2 1 0 77 if strncmp namptr, msgptr2 dest spec p name, MAXHAM 0 78 msgptr2 IPCSD TXRXMSG msgptr2 links next 79 genr ulk fdisv hldhead, fdisv hldtail, msgptr2 80 msgptr2 ipcmsgtype inc TXSTAI 81 msgptr2 txrx.txstat.status FAILURE 82 ifcmm txs msgptr2 83 msgptr2 msgptr3 84 else 85 msgptr2 IPCSD TXRXMSG msgptr2 links.next 86 genr free msgptr 87 88 PROGRAM LISTING D 10 MODULE NAME 11 12 SUBSYSTEM NAME NIM 13 14 DATE TIME OF CREATION XE XU 15 16 DATE TIME OF COMPILATION XD XT 17 18 VERSION 19 20 PROGRAMMER Bernie Weisshaar 21 22 DESCRIPTION 23 Function to accept and deal with message received off LAN.24 25 26 PARAMETERS PASSED 27 rxmsg address of IPCSD TXRXMSG structure for received msg 28 29 30 PARAMETERS RETURNED None 31 32 33 34 static char Sreid XE XM XI 35 36 37 38 num include local ex.h 39 num include kerntypes.h 40 num include niekern.h 41 include inimgen.h 2 num include loal nim nimif.h 43 44 num define self hwv cettaddr 45 46 47 48 void ifir rxmeg rxmsg 49 50 register IPCSD TXRXMSG rxmsg 51 52 53 extern void genr free 54 extern uneigned int ifor no 55 extern void ifcpmr fxo 56 extern bool ifcpmr nam 57 extern unsigned int hwv celladr 58 extern long iftov pid 59 extern long ifdisv pid 60 extern Long nwev pid 61 register IPCSD TXRXMSG rxmsg2 62 register MSG esgadd 63 64 if rxmsg mesorg self 6 If rxmsg msgtype MSG TYP CPMMEG amp MSG rxmsg megaddr mode Unsigned char DIRECT 66 ifcpmr rxm rxmsg 67 else 68 num ifdef OLDKERN 69 70 Wetse Put DIRECT, iftov pid, rxmsg, MAXPRI 71 72 num endif 73 else 74 switch rxmsg msgtype 75 case MSG TYP CPMMSS 76 msgadd HEG rxmsg msgaddr 77 switch msgadd mode C 78 case DIRECT 79 i fcpmrrxm rxmsg 80 break 81 case NEXT 82 case ALL 83 case LOCAL 84 if ifcpmer nam xmsg dest spec.p name 85 rxmsg ipcmsgstype int IXMEG 86 rxmsg msgtype HSG TYP CPMREJ 87 num ifdef OLDKERN 88 Put DIRECT, mwov pid, rxmsg, MAXPRI 89 num else 90 Put DIRECT, nwovpid, rxmsg 91 mendif 92 else 93 rxmsg2 IPCSD IXQXMSG A loc sizeof IPCXD TXRXMGG rxmsg msglen 94 rxmsg2 2rxmsg 95 rxmsg2 ipcmsgtype int TXMSG 96 rxmsg2 single TRUE 97 rxmsg2 msgaddr unsigned char rxmsg2 1 98 genr copy rxmsg msgaddr, rxmsg2 msgaddr, rxmsg msglen 99 num ifdef OLDKERN 100 Put DIRECT, ifdisv pid, rxmsg2, MAXPRI 101 else 102 Put DIRECT, ifdisv pid, rxmsg2 103 num endif 104 ficpmr rxm rxmsg 105 1 106 break 107 108 break 109 case MSG TYP CPMRE 110 num ifdef OLDKERN 111 Put DIRECT, ifdisv pid. rxmsg, MAXPRI 112 num else 113 Put DIRECT, ifdisv pid, rxmsg 114 num endif 115 break 116 case MSG TYP DIS 117 rxmsg ipcmsgtype int TXMSG 118 if ifcpmr nam rxmsg 1 119 rxmsg msgtype REG TYP DISREP 120 rxmsg msgercdst rxmsg msgorg 121 num ifdef OLDKERN 122 Put DIRECT, nwov pid, rxmsg, MAXPRI 123 num else 124 Put DIRECT, nwov pid, rxmsg 125 num endif 126 1 else 127 if rxmsg msgercdet ifor nc i hwv celladdr 128 num ifdef OLDKERN 129 Put DIRECT, nwov pid, rxmsg, MAXPRI 130 num else 131 Put DIRECT, nwov pid, rxmsg 132 num endif 133 else 134 oenr free rxmsg 135 break 136 case MSG TYP DISRSP 137 num ifdef OLDKERN 138 Put DIRECT, iftov pid, rxmsg, MAXPRI 139 num else 140 Put DIRECT, iftov pid, rxmsg 141 num endif 142 break 143 case MSG TYP UPD 144 ifdef OLDKERN 145 Put DIRECT, ifdisv pid, rxmsg, MAXPRI 146 num else 147 Put DIRECT, ifdiev pid, rxmsg 148 num endif 149 break 150 151 PROGRAM LISTING E 10 MODULE NAME 11 12 SUBSYSTEM NAME NIM 13 14 DATE TIME OF CREATION XE XU 15 16 DATE TIME OF COMPILATION XO XI 17 18 VERSION 19 20 PROGRAMMER Bernie Weisshaar 21 22 DESCRIPTION 23 Process to keep track of which visible processes reside on 24 this cell. 25 26 27 PARAMETERS PASSED None 28 29 30 PARAMETERS RETURNED None 31 32 33 34 static char sreld X2X XMX XI 35 36 37 38 num include local ox.h 39 num include local ox clock.h 40 num include nimkern.h 41 num include nisgen.h 42 num include local nim nimif.h 43 44 num define RECPERSEG 20 45 46 47 struct SignalMsg if1 signalmsg 48 everyien , 49 0, year 50 0, month 51 0, day 52 0, day of week 53 0, hours 54 0, minutes 55 10, seconds 56 ifconp 57 58 59 60 struct SingleMsg if2 signalmsg 61 cance .62 0, year 63 0, month 64 0, day 65 0, day of week 66 0, hours t 67 0, minutes 68 10, seconds 69 ifcomp 70 71 72 73 int trg ifcomp eg 74 int trg ifcomp lg 75 76 77 num ifdaf CXE 78 int ifv offset effect to name field 79 long hwchamgr pid pid of hwchamgr 80 long hwchbmgr pid pid of hwchbmgr 81 long hwchcmgr pid pid of hwchcmgr 82 long nwov Dia pid of nwop 83 int hwv celladdr this cell s physical address 84 num else 169 hwchamgr pid IPCSD INIT ipcmsg hwchamgr pid 170 hwchamgr pid IPCSD INIT ipcmsg hwchbmgr pid 171 hwchamgr pid IPCSD INIT ipcmsg hwchcmgr pid 172 nwov pid IPCSD INIT ipcmsg nwop pid 173 hwv celladdr IPCSD INTI ipcmsg hwv celladdr 174 num endif 175 Free ipcmsg 176 genr prnt ifconp initia ized n 177 initialized TRUE 178 ipcmsg int Alloc sizeof struct SignalMsg 179 struct SignalMSg ipcmsg if sighalmsg 180 num ifdef OLDKERN 181 Put NEXT, signal , ipcasg, MAXPRI 182 Aelse 183 Put NEXT, signat , ipc asg 184 num endif 185 signalon TRUE 186 break 187 case ECHO 188 Reply ipcmsg, ipcmsg 189 break 190 case INT start of code for INT 191 printf si gnalonn , signalon 192 printf nitconv nghead x, ifconv ngtail x n , ifconv nghead, ifconv ngtail 193 if ifconv nghead 1 0 194 printfc count sent process name n 195 for ptr ifconv nqhead ptr 0 ptr IFCONSD NAMQUE ptr links.next 196 printf Z3d ld Zs n , ptr c et, ptr sent, ptr name 197 198 Free ipcmsg 199 break end of code for INT 200 case CONCRE 201 ptr genr nam fconv nghead, IPCSD CONTEXT ipcmsg name, ifv offset 202 if ptr 1 0 203 Free ipc asg 204 else C 206 ptr count 0 207 ptr sent FALSE 208 genr copy IPCSD CONTEXT ipcsmg name, ptr name, MAXNAM 209 genr alk fconv nghead, fconv ngtail, ptr, int ptr name int ptr 210 Free ipcmsg 211 if initialized 212 if destin ifor nc huv celladdr C 213 tptr IPCSD TXRXMSG Alloc sizeof IPCSD TXRXMSG MAXNAM 214 tptr ipcmsgtype int TXAMSG 215 tptr msgaddr unsigned char tptr 1 216 tptr msglen MAXNAM 217 tptr msgtype MSG TYP UPD 218 tptr msgpri MAXPRI 219 tptr msgorg hwv celladdr 220 tptr msgsrcdst destin 221 tptr single TRUE 222 genr copy ptr name, tptr 1, MAXNAM 223 num ifdef OLOKERN 224 Put DIRECT, nwov pid, tptr, MAXPRI 225 else 226 228 ptr. sent TRUE 227 num endif 229 else 230 if lsignalon 231 ipcmsg int Alloc sizeof struct SignalMsg 232 struct SignalMsg ipcmsg ifl signalmsc 233 num ifdef OLDKERN 234 235 else Put NEXT, signal , ipcmsg, MAXPRI 236 237 num endif 238 signalon TRUE 239 240 241 ptr count tt 242 break 243 case IFTO 244 ptr genr nam fconv nghead, IPCSD TXRXMSG ipcmsg 1, ifvoffset 245 if ptr 0 246 genr free ipcmsg 247 else 248 if destin ifor nc 1 hwv celladdr 249 IPCSD TXRXMSG ipcmsg ipcmsgtype int TXMSG 250 IPCSD TXRXMSG ipcmsg msgstcast destin 251 Forward DIRECT, nwov pid, ipcmsg 252 Ptr sent TRUE 253 else 85 extern int ifv offset, offset to name field 86 extern long hwchamgr pid pid of hwchamgr 87 extern long hwchbmgr pid pid of hwchbmgr 88 extern long hwchcmgr pid, pid of hwchcmgr 89 extern long nwov pid pid of nwop 90 extern int hwv celladdr this cell s physical address 91 Aendif 92 93 94 95 PROCESS ifconp 96 C 97 extern void genrcopy 98 extern void genrprnt 99 extern void genralk 100 extern void genrlk 101 extern void genr ulk 102 extern void genr free 103 extern IFCONSD NAMQUE genr gar 104 extern IFCONSD NAMQUE genr nam 105 extern unsigned int ifornc 106 107 IFCONSD NAMQUE ifconv avhead avaitable queue head pointer 108 IFCONSD NAMQUE ifconv avtail avaitable queue tail pointer 109 IFCONSD NAMQUE ifconv nghead resident process name queue head pointer 110 IFCONSD NAMQUE ifconv ngtail resident process name queue tail pointer 111 register IFCONSD NAMQUE patr pointer into resident process name queue 112 register IPCSD TXRXMSG tptr pointer to txmsg IPC message 113 register int ipcmsg pointer to received IPC message 114 unsigned int desting addr of cell to be sent UPD message 115 bool initialized flag indicating receipt of INIT message 116 bool signalon flag indicating whether signating enabled 117 118 ifconv nghead ifconv ngtail 0 119 ifconv avhead ifconv avtail 0 120 signaton initialized FALSE 121 122 FOREVER C 123 trg ifconp eg 1 124 num ifdef OLDKERN 125 ipcmsg Get NULL, NULL 126 num else 127 ipcmsg int Get NULL, NULL, NULL 128 Aendif 129 trg ifconp lg ipcmsg 130 if strcmp awake , ipcmsg 0 131 Free ipcmsg 132 genr prnt ifconp awake n 133 if destin ifor nc hwv celladdr 134 for ptr iftonv nghead ptr 0 ptr IFCONSD NAMQUE ptr links.next 135 if lptr sent 136 tptr IPCSD TXRXMSG Alloc sizeof IPCSD EXRXMSG MAXNAM 137 tptr ipcmsgtype int TXMSG 138 tptr msgaddr unsigned char tptr 1 139 tptr msglen MAXNAM 140 tptr msgtype MSG TYP UPD 141 tptr mstpri MAXPRI 142 tptr msgorg hwv celladdr 143 tptr msgsrcdst destin 144 tptr single TRUE 145 genr copy ptr name, tptr 1, MAXNAM 146 num ifdef OLDKERN 147 Put DIRECT, nwov pid, tptr, MAXPRI 148 num else 149 PUt DIRECT, nwov pid, tptr 150 num endif 151 ptr sent TRUE 152 153 if signalon C 154 ipcmsg int Alloc sizeof struct SignalMsg 155 struct SignalMsg ipcmsg if2 signalmsg 156 num ifdef OLDKERN 157 Put NEXT, signal1,, ipcmsg, MAXPRI 158 num else 159 Put NEXT, signal , ipc sg 160 num endif 161 signalon FALSE 162 163 164 3 else C 165 switch ipcmsg C 166 case INIT 167 num ifdef CXE 168 ifv offset IPCSD INIT idcmsg ifv offset 254 gnr free ipcasg 255 ipcmsg int Alloc sizeof struct SignalMsg 256 struct SignatHsg ipcmsg if1 signalmsg 257 num ifdef OLDKERN 258 Put NEXT, signal , ipcgmsg, MAXPRI Z59 num else 260 Put NEXT, signal , ipcmsg 261 num endif 262 signalon TRUE 263 ptr sant FALSE 264 265 break 266 case CONDEL 267 prt genr num fconv nghead, IPCRD CONTEXT ipcmsg name, ifv offset 268 if ptr I 0 269 if ptr count 0 270 genr ulk fconv nghead, fconv ngtail, ptr 271 genr lk fconv avhead, fconv avtail, ptr 272 273 Free ipcmsg 274 break 275 case NAMESRCH 276 Ptr genr num fconv nghead, IPCSO CONTEXT ipcmsg name, ifv offset 277 IPCSD MANESRCN ipcmsg exists ptr 0 278 Repty ipcmsg, ipcmsg 279 break 280 default 281 Forward NEXT. error . ipcmgs 282 break 283 284 285 286