# APPARATUS FOR DETERMINING POLES AND ZEROES OF THE TRANSFER FUNCTION OF A DEVICE

## Claims
Vorrichtung zum Bestimmen von Polen und Nullstellen der Übertragungsfunktion eines Gerätes, umfassend

## Description
The present invention relates to an apparatus for determining poles and zeroes of the transfer function of a device in accordance with the preamble to claim 1. Various apparatus have been constructed in the prior art which measure the transfer function of a linear device U.S. Patents 3,973,112 and 4,047,002 . An apparatus in accordance with the preamble to claim 1 is known from the Fourier Analyzer Training Manual , Hewlett Packard Application Note 140 0, available from Hewlett Packard Company, Palo Alto, California. The transfer function of a linear device describes the behavior of the device in the Laplace transform s plane domain in response to an applied stimulus. In the design of electrical, mechanical or electromechanical devices it is often important to know the location and strength of each pole and zero of the device transfer function in the s plane so that the device may be optimized for a desired application. In particular, where device stability is important, knowledge of pole locations is critical. In order for the pole and zero measurement to be of use to the device designer, it is important that the measurement be performed quickly and accurately and that errors due to noise and distortion be minimized. By means of the characterizing features of claim 1, the present invention devises an apparatus in accordance with the claim preamble wherein improved accuracy is obtained relative to the state of the art, which leads to a lower risk of erroneously determinating a location of a pole of the transfer function in the wrong half plane of the s domain. This is advantageous in the analysis of servo systems and other systems where instability is to be avoided. In accordance with the illustrated embodiment of the present invention, a pole and zero analyzer is disclosed which performs the pole and zero measurement quickly and accurately and which corrects for noise induced errors in the measurement. The pole and zero measurement is initiated by the application of a desired stimulus signal, such as random or Gaussian noise, to the device. The stimulus and response signals are sampled in the time domain and are transformed to the frequency domain via a Fast Fourier Transform so that the auto power spectrum of the stimulus signal and the cross power spectrum of the stimulus and response signals may be measured. In order to minimize pole zero measurement error caused by noise and nonlinearities in the device response, an ensemble of stimulus and response signal measurements may be made and the auto power and cross power spectra may be measured as ensemble averages. The measured transfer function of the device is determined from the cross power and auto power spectra and the noise level on the measured data is estimated from the ensemble averages of the stimulus and response signals. Measurement time delays may be removed as required. The measured transfer function is fitted to an estimated transfer function comprising a rational fraction of numerator and denominator Chebyshev polynomials in the variable, s. A weighting function is determined which may be used to emphasize certain portions of the transfer function for the purpose of increasing the accuracy of the pole and zero measurements. The coefficients of the numerator and denominator polynomials are then found as the weighted least squares fit of the measured transfer function data to the estimated transfer function. The quality of the fit of the estimated transfer function to the measured transfer function is determined. If the fit is insufficient, the orders of the numerator and denominator polynomials are varied, new coefficients are determined and the fit is again tested. When a sufficient fit is achieved, the Chebyshev numerator and denominator polynomials of the estimated transfer function are converted to ordinary polynomials and a root solver is utilized to find the roots of the two polynomials which yield the poles and zeroes of the estimated transfer function. The poles and zeroes may be displayed as desired by the device designer. An embodiment of the invention will now be described in detail with reference to the accompanying drawings. Figure 1 shows an analyzer 1 which is constructed in accordance with the present invention. A stimulus signal x t is applied through a cable 9 to a device 3 and the response y t of device 3 is received by analyzer 1 through a cable 11. Device 3 may be any electrical, mechanical or electromechanical apparatus, e.g., a servomotor. If external transducers are used, it is possible to apply mechanical vibrations to device 3 and to monitor the response of device 3 thereto for modal analysis. Various instructions may be input to analyzer 1 by the user through a keyboard 5 and the results of the measurement may be displayed on a display 7 such as a CRT. Figure 2 is a block diagram of analyzer 1. An analog source 27, driven by a digital source 39 under user control through keyboard 5, generates a stimulus signal x t which is applied through cable 9 to device 3. Both the stimulus signal x t and the response signal y t are received through input channels 21, 23 and are digitized by analog to digital converters 29, 31 at a sample rate set by local oscillator 37. A digital filter 33 may be used to reduce aliasing errors on sampled x t Figure 3 is a flow chart of the operations performed by analyzer 1 during the measurement of the poles and zeroes of the transfer function of device 3. In initial step 61, stimulus signal x t is applied to device 3 and response signal y t is recorded. The two signals are digitized and transformed to the frequency domain as X f In step 67, a recurrence relationship is utilized to generate the numerator and denominator Chebyshev polynomials, P s and Q s , of the estimated transfer function, H The various steps of the flow chart shown in Figure 3 will be discussed hereinbelow individually and in detail. In step 61, the actual transfer function of device 3 is measured. Figure 4 shows, in sequence, the individual steps of step 61. A stimulus signal x t is applied to device 3 and both x t In step 95, the time domain signals x t It is possible that time delays may exist in the measured data because of the physical constraints under which the measurements are made or because of non ideal transducers or other devices used. These time delays should be removed from the data so that undesired virtual poles and zeroes are not injected into the analysis. Removal may be accomplished by any of a number of methods such as by multiplication of the frequency domain data by e The measured transfer function, H The transfer function of device 3 is defined, in terms of the auto power and cross power spectra as wherein the In order to minimize the effect of the noise term on the measurement of the transfer function, the auto power and cross power spectra may be determined as ensemble averages over a number of stimulus response measurements. As the number of measurements in the ensemble is increased, the noise term averages to zero because of the random phase angles of the uncorrelated noise term. If random noise is used for the stimulus signal, then this averaging has the additional effect of linearizing non linearities in the response of device 3 because of the lack of correlation between the stimulus signal and any non linear response signals. It has been found that an ensemble of between 10 and 1000 measurements has yielded good results, and the number of measurements in the ensemble may be selected by the user. Using the ensembles, the transfer function of device 3 may be measured as the quotient In step 107 of Figure 4, H Although the variance may be estimated by resort to any of a number of well known methods, the variance may be determined rigorously in accordance with equations 4.92 and 4.93 at page 131 of the March, 1957, New York University Ph.D. dissertation by N.R. Goodman entitled On The Joint Estimation Of The Spectra, Cospectrum And Quadrature Spectrum Of A Two Dimensional Stationary Gaussian Process . The equations may be rewritten as Bessel functions of the second kind and may be integrated to yield wherein n is greater than one and n is the number of measurements in the ensemble. The coherence function used in equation 8 is defined as The user has the option of entering a user defined weighting function, through controls 5. The user defined weighting function allows the user to emphasize certain desired regions along the frequency axis in light of areas of known high or low measurement accuracy or of particular importance to the user. In that case, the user defined weighting curve is displayed on display 7 and step 65 need not be performed. Otherwise, in step 65 of Figure 3, a weighting function W f for use in the least squares fit analysis is generated. Figures 6 and 7 each show various steps, of step 65, which may be performed to create one of two weighting functions. The weighting function generated by the steps shown in Figure 6 emphasizes regions along the frequency axis in which H The total weighted error ε , as defined in the least squares fit analysis is wherein the error and weighting functions are evaluated at each frequency point and the summation is taken over the frequency range of interest. The weighting function, W f , is used to emphasize desired regions of f in the least squares fit analysis. H If it is assumed that the coefficients of P s are c₀, c₁, ..., c Steps 121 137 of Figure 6 show in detail the creation of the weighting function, W f , outlined at step 67 of Figure 3. In essence, W f is the phase derivative of H In steps 105 107 of Figure 4, the real and imaginary components of the complex frequency domain measured data points of H But, only the phase information is desired, so Thus, it is necessary to evaluate the phase of x₃x₁ . Use of the small angle approximation is convenient and allows computation time to be minimized. The approximation that ph x₃x₁ is equal to tan ¹ ph x₃x₁ may be used in the region of 2 tan ¹ ph x₃x₁ 2 wherein tan ¹ ph x₃x₁ is measured in radians. Outside of the region, tan ¹ ph x₃x₁ is approximated as 2 or 2. An added benefit of the small angle approximation is that noise on H Since the phase derivatives obtained at step 121 tend to be noisy, the phase derivative is smoothed at step 123. Smoothing may easily be performed by averaging the individual phase derivative data points with the seven phase derivative data points on either side of the particular data point of interest. Thus, each point on W f is really an average of fifteen contiguous phase derivative data points with the result that noise spikes are reduced. While seven points on either side have been used, it has been found that the number may be varied from five to fifteen as desired with good effect. For servo analyses, thirteen points on either side have been used because of the high noise and light damping characteristics of the data. For modal analyses, seven points on either side have been sufficient because of the low noise and highly damped data. Because only a positive weighting function is desired, the absolute magnitude is taken in step 125. The square root may be taken at step 127. In servo analyses, a smoother weighting function is usually desired so that all data may be used and the square root is often taken. Conversely, in modal analyses, the square root is usually not taken because of the typically higher Q factor of the devices being analyzed. In step 129, another smoothing may be made of the weighting function. Smoothing is performed in the same manner as in step 123. It is also desirable to correct the weighting function to minimize the effect of regions having high noise on the measured data. The coherence function, measured in step 111 of Figure 4 and defined in equation 9, provides an estimate of the noise on each measured frequency domain data point of H de emphasizes regions of high noise where the coherence function approaches zero. Use of the .05 factor allows clean data points to be emphasized by a factor of 20 greater than noisy data points. The resulting quantity, including the absolute magnitude of the phase derivative, at each point along the frequency axis, is In step 133, the noise corrected weighting function is normalized to 1 to avoid numerical overflow and in step 135 a threshhold of .02 is injected so that no data is totally disregarded. The weighting function is, thus, defined as Finally, in step 137, the user had the option of modifying W f . W f , varying from 0 to 1, is displayed upon display 7 of analyzer 1 as a function of frequency. If no modification is made by the user, analyzer 1 starts at the highest and lowest frequency points and scans toward the center of the frequency range until a W f of at least 0.04 is found at both ends. Data above and below these two end points are ignored. In order to decrease measurement time or in order to avoid elimination of certain desired data, the user can specify the end points by means of two cursors on display 7 and controlled by controls 5. Figure 7 shows the various steps, of step 63, which may be used in lieu of the steps shown in Figure 6 to generate an alternative weighting function. This alternative weighting function may be preferable in modal analysis because of the noise and distortion which often occurs near zeroes in many modal analysis situations. Modal analyses often involve sharper peaks than those involved in most servo analyses. The alternative weighting function tends to locate peaks very accurately and both pole and zero information may be derived therefrom. In step 141, the squared value of the measured transfer function data H The amplitude of each data point is then subtracted from the amplitude of the associated line segment. The remainder is compared to a threshhold value to determined whether or not a valid peak has been detected. A typical threshhold of In step 147, a weighting function parabola is constructed around the center of each peak determined in step 145. The parabola center is located at S T 2 wherein S and T are each three points beyond the outermost data points at which the threshhold was exceeded for the peak. The maximum height of the parabola is 3L 2 T S above the baseline wherein L is the total number of frequency data points. Thus, the parabolas are constructed with width inversely proportional to height so that neither overly wide nor overly narrow peaks are ignored. In step 151, the weighting function is normalized by dividing through by the maximum peak value. In step 153, the squared magnitude of the original measured data of H In step 67 of Figure 3, the numerator and denominator basis polynomials of the estimated transfer function, H In step 69 of Figure 3, the coefficients of P s and Q s are determined. Figure 8 shows the various steps of step 69 in greater detail. The point by point error term defined in equation 11 may be rewritten as wherein the f notation has been dropped for the sake of convenience and the j subscripts indicate that sampled data along the frequency axis are being used. Multiplication through by the denominator polynomial Q s yields Using equation 6, and defining new terms A for simplicity, equation 23 may be rewritten as Multiplication through by B In step 161 of Figure 8, the user prespecified poles and zeroes may be removed from the estimated transfer function. Although this is not necessary, prespecification by the user of known poles and zeroes allows the analysis of unknown poles and zeroes to be performed with additional accuracy. Known zeroes are removed from the P s polynomial and known poles are removed from the Q s polynomial by reference to wherein z The W Thus, equation 32 may be simplied and rewritten as In step 163 of Figure 8, the measured data, weighting functions and estimated transfer function polynomials are converted to matrix notation for ease of analysis utilizing the analyzer 1 shown in Figures 1 and 2. This conversion may easily be performed by persons of ordinary skill in the art. Two sets of orthogonal polynomials, Because the Chebyshev polynomials are data independent and therefore do not have to be redetermined for each measurement , it is preferable to construct Matrix and vector notation is as shown in Figure 9. By inspection of the ϑ matrix, the first column comprises T₀ f wherein T And, the error equation 34 may be rewritten as Now, the sum of the squared errors may be written in matrix notation as wherein the T superscript denotes the complex conjugate transpose. In step 165 of Figure 8, the least squares fit analysis discussed hereinabove is constructed in matrix form. The goal is to minimize with respect to the elements of the C and D vectors. This minimization is accomplished by differentiating ε with respect to each element of C and D, by then setting the results equal to zero and solving for the satisfying C and D vectors. The resultant equations are given as For the sake of clarity, equations 42 and 43 may be rewritten as In step 167 of Figure 8, the noise bias is removed from the least squares fit analysis. From equation 33, the W The coherence function and the variance are defined hereinabove in equations 8 and 9 and are obtained from the three spectra measurements made by the analyzer 1. Since the variance is the noise power in terms of squared magnitude, it may easily be subtracted from the W In step 169 of Figure 8, solution matrices are constructed and filled to determine the Chebyshev polynomial coefficients for the minimization of the least squares fit analysis. Equations 42 and 43 may be rewritten as equations 44 and 45 wherein noise bias is presumed to have been removed as described above if desired. New functions F, H, H such that the relations, In the case where an analysis of the maximum possible number of poles and zeroes is being made m n 40 , each of the D and C vectors is 40 elements long and the M matrix contains 6400 elements. Storage of this large number of elements is inefficient and the Chebyshev product sum relationship described hereinabove may be utilized to minimize the size of the required memory and the amount of time required to regenerate the solution matrix, M. When this data compression is accomplished, the number of elements required to regenerate matrix M is reduced from 6400 to 320. Further, it may be recognized that the third quadrant of the M matrix is merely the transpose of the second quadrant so that only one or the other need ever be fully regenerated for solution of the matrix. The other may then be reconstructed easily from it. And, in addition, only two of the four matrix quadrants need be regenerated at any one time during the solution of the matrix. The Chebyshev product sum relationship states that may be used in the data compression discussed above. The result is that each element of the M matrix is a summation of two other summations in the matrix. From Figure 10, it can be seen that the column vector, V, must be orthogonal to each row of the matrix, M. Thus, M₁V 0, M₂V 0, ..., M In step 171 of Figure 8, the rows of matrix M are orthogonalized relative to each other. Figure 11 shows the various steps comprising step 171 in detail. In step 193, the row of M which has the smallest magnitude diagonal element is selected and deleted from matrix M. The resulting matrix M is now an n 1 by n matrix. One row is deleted so that it is possible to have an n length vector V which is orthogonal to the n 1 rows of M. It is desirable to delete that row which is most contaminated by noise so that the deletion has only a minimal effect on the accuracy of the performance of the analyzer 1. Since the basis polynomials being used are Chebyshev polynomials, the diagonal element of each row may be viewed as an estimate of the power of the particular Chebyshev term represented by that row wherein each row comprises a Chebyshev polynomial having an order equal to the row number. By deleting the row having the smallest diagonal value, the row having the least power and, therefore, the largest relative amount of noise contamination, is deleted. The steps 195 203 of step 191 comprise the orthogonalization of the remaining n 1 rows of matrix M relative to each other using the Gram Schmidt technique. A detailed description of the Gram Schmidt orthogonalization technique may be found, e.g., at page 256 of the above referenced Ralston and Rabinowitz textbook. As described therein, the elements of each row are removed successively from each succeeding row in a least squares sense. The result of the orthogonalization technique is that the n 1 rows are orthogonal to each other. Thus, if column vector V of length n is viewed as the n th row of the n by n matrix made up of vector V and the n 1 rows of M, then a vector V which is orthogonal to the n 1 rows of M does exist. Therefore, the solution does exist. In step 173 of Figure 8, a random number generator is used to generate random elements of vector V. In step 175, the GramSchmidt orthogonalization technique described hereinabove is used to remove each of the n 1 orthogonalized rows of M from vector V. When step 175 is completed, all of the n rows n 1 rows of M plus the vector V are mutually orthogonal. There is a very slight possibility that the randomly generated initial vector V could be a linear combination of one or more of the n 1 orthogonalized rows of matrix M. In step 177, the orthogonalized vector V is tested for validity. If V were, in fact, generated as a linear combination of one of the n 1 rows of M, then the orthogonalized vector V would be very small. If this is the case, then V is determined invalid and another random vector is generated. An empirically successful criterion for determining validity is to evaluate the magnitude of the orthogonalized V vector and compare it to the roundoff noise level of the processor 45. If the magnitude of V approaches the roundoff noise level, then it is determined to be invalid. For greater accuracy, for a 16 digit processor 45, vector V is determined invalid if its squared magnitude is found to be less than 10 ¹². Figures 12A B show the various steps of the automatic order selection which is performed by analyzer 1 and which is shown at step 231 of Figure 3. Step 231 shows a gross overview of the function of the automatic order selection which is shown in much greater detail in Figures 12A B. The goal of order selection is to increment the pole and zero orders n and m until a fit of H In step 251, the fit criteria for determination of a good fit are determined. There are a number of criteria which may be used depending upon the particular analysis being performed. One is to test the quality of fit point by point through the frequency range and determine that a good fit is achieved if the total number of points which lie outside of an error limit is less than an acceptance number of points. The first step in computing the error limit is to calculate the normalized variance at each frequency point from the variance measured above. The point by point normalized variance is defined as the variance at each point divided by the magnitude of H Since the error limit is based on the variance, smoothing may be desired to eliminate random fluctuations. Thirteen point smoothing average of each point with the six points on either side may be used. In order to preserve peaks in the original error limit, the greater of the original error limit or the smoothed error limit may be selected point by point for use as the final error limit. The acceptance number, AN, is the maximum number of points within the frequency range which may be outside of the error limit while still having a good fit. This number may be determined empirically or may be used. F In step 253, an initial order estimate of m n 1 is made. It should be noted that the user has the option of specifying an initial order estimate other than 1. The user also has the option of specifying m and n so that the steps of Figures 12A B are not performed at all. The user can also specify m In step 255, the matrix M is reconstructed using the initially estimated orders of m and n. Reconstruction may be made directly if the full matrix parameters were stored. If data compression was used, then reconstruction from the retained maximum 320 parameters may be had with reference to equations 53 and 54 discussed above. Once the matrix is reconstructed, the coefficients of the D and C vectors shown in Figures 9 and 10 are solved for as discussed above. In step 257, the estimated transfer function, H If the fit is determined to be good, then the current values of m and n are stored as m In steps 279 285, m and n are held to maximum limits which may have been imposed by the user or by the maximum limit of 20 by the analyzer 1. If both m and n are at the maximum limits, then the values of m and n are stored at step 289. If not, step 255 is re executed and another fit using the incremented m and n is evaluated. Thus, m and n are incremented until either a good fit is achieved or until the maximum limits of both m and n are reached. In steps 301 303, if the number of zeroes, m, is greater than 0, then m is decremented since both servo and modal systems typically have at least one more pole than zero. Thus, a more accurate fit may be achieved by reducing the number of zeroes and reevaluating the resulting fit. The best fit is deemed achieved at the number of zeroes for which the next two lower numbers of zeroes both have inadequate fits. If, in step 303, it was found that m was equal to zero, then step 333 would be executed next. If not, in steps 305 307, the matrix M is reconstructed for the current values of m and n, the Chebyshev coefficients are solved for and the fit of H In step 309, the fit is evaluated against the fit criteria. If the fit is good, then m and n are stored and m is again decremented at step 301. If the fit is not good, and if m 0, step 333 is executed. If m is not 0, then m is again decremented in step 315. In steps 317 and 319, a new solution is made and a new fit is evaluated. If the fit is now good, m and n are stored and m is decremented at step 301. If, on the other hand, the fit is again bad, m is reset at step 333 to m Figures 13A B show the various steps of an alternative automatic order selection which may be used at step 231 of Figure 3 in lieu of the steps shown in Figures 12A B. In overview, the steps of Figures 13A B may be divided into a number of serial decision blocks which determine whether or not to vary the polynomial orders and to determine new polynomial coefficients. In block 531, the numerator order, m, is incremented and new coefficients are determined if a previous reduction in the order of m caused a degradation of the error to signal ratio. In block 533, both orders m and n are incremented and new coefficients are determined if the error to signal ratio exceeds the noise to signal ratio by a predetermined amount indicating that the error is not caused solely by the noise on the measured data. In block 535, m is reduced, and new coefficients are determined, if it is determined that the higher order coefficients have less than a certain minimal effect on the measured data. In block 537, new data is taken and the entire pole zero analysis is performed again if an inadequate fit is obtained despite attaining the maximum allowed order n. Finally, in block 539, the fit may be determined to be sufficiently good. If the fit is determined conditionally good in blocks 531 537 and the orders m and n are equal, then the fit is considered sufficiently good. Alternatively, if the error is determined not to be excessively biased then the fit is also considered to be sufficiently good. Otherwise, both m and n are incremented and new coefficients are determined. In step 401 of block 531, a determination is made of whether or not the order m of numerator P s has ever been reduced during the current performance of step 231 of Figure 3. A flag U0 is set dependent upon the outcome of the determination. If m had previously been reduced, a weighted error to signal ratio, V0, is calculated in step 405 and is evaluated in step 407. V0 is defined as If V0 is 5 or more greater than the V0 stored as V1 from the prior fit evaluation with the higher m, then both m and the threshhold S1, discussed below are incremented and step 69 is repeated. The determination made in step 407 indicates that the quality of fit measured by the error to signal ratio degraded when m was reduced so m is incremented back to its previous value and step 69 is repeated. In step 431 of block 533, assuming that the error to signal ratio did not increase by 5 or if no reduction of m had previously been made, the noise to signal ratio, R, is calculated. R is defined as using the definition of variance as the noise power from equation 8 above and using the denominator from equation 57 above. In step 441, the noise to signal ratio is compared to a multiple of the error to signal ratio. The multiple, S0, is used to correlate the quality of fit to the noise on the measured data. S0 has been empirically chosen to be 2, although S0 may be increased in step 465 to allow a looser fit because of higher noise on the measured data. If the error to signal ratio V0 exceeds the noise to signal ratio, R, by more than a factor of S0, then the fit is considered to be poor. If the denominator order is less than the maximum allowed, then both m and n are incremented by 1 and step 69 is repeated. This increases the orders of both P s and Q s to allow a tighter fit of the estimated transfer function to the measured transfer function data. The quality of fit should increase until m or n exceeds the true order of the measured transfer function data. In analyzer 1, the maximum permissible order of each of m and n is 20 a total of 40 each for positive and negative frequencies combined . If, in step 443, the maximum denominator order of 20 is reached, then block 535 is entered. Also, if the quality of fit was conditionally determined to be good in step 441, then block 535 is entered. In step 449 of block 535, a threshhold level S1 is determined. S1 is defined such that wherein c wherein the new weighting function L s is defined as In L s , the W s term allows for emphasis of peaks, the Q s term removes the effect of the denominator and the H In steps 451 and 453, the numerator order m is decreased to the threshhold order, S1 1. It should be noted that the denominator order, n, is never reduced in the flow chart of Figures 13A B. The numerator order is reduced to the threshhold in order to ensure that no virtual zeroes are injected by maintaining the numerator at too high a level. S1 is determined so that only relatively ineffective coefficients of P s are eliminated so that the order of P s is not reduced too far. The factor of 1.5 in equation 59 may be varied to ensure that an optimum number of zeroes are fit. In step 461 of block 537, the comparison of the noise to signal ratio and the error to signal ratio of step 441 is repeated. If the error to signal ratio exceeds the predetermined multiple of the noise to signal ratio and the maximum allowed order n has been met or exceeded, then a determination is made that the measured data is contaminated. If so, the entire pole zero analysis is performed again with an increased allowed multiple of the noise to signal ratio, S0, to account for the contaminated data. If the maximum allowed denominator order is not met or exceeded, then step 491 of block 539 is performed. If the numerator and denominator orders are not equal then the fit is determined to be sufficiently good, the steps of Figures 13A B are completed and step 77 of Figure 3 is performed. If the numerator and denominator orders are equal, then a final check on the quality of the fit is performed. If the check is successful, or if unsuccessful and the denominator order is equal to or greater than the allowed maximum order, then the fit is determined to be sufficiently good. If the check is unsuccessful and the denominator order is less than the maximum allowed, then both the numerator and denominator orders are incremented and new coefficients are determined at step 69. An optional final check, at step 493 of block 539, is an analysis of the polarity of the error between H In step 335, the Chebyshev coefficients are solved for after the matrix M has been reconstructed for the already determined best fit orders of m and n. Conversion to ordinary polynomials as shown in step 77 of Figure 3 may now be accomplished. In step 77 of Figure 3, the P s and Q s Chebyshev polynomials of H In step 79, a root solver is used to find the roots of P s and Q s which, by definition, are the zeroes and poles of H Finally, in step 81 of Figure 3, the poles and zeroes of the estimated transfer function H