# ADAPTIVE METHOD FOR CHARACTER DATA COMPRESSION

## Claims
Ein anpassungsfähiges Verfahren zur Komprimierung von Zeichendaten, in welchem diese Daten in Wörter und Trennzeichen unterteilt werden und das folgende Schritte umfaßt

## Description
This invention generally relates to a method of compressing character data and, more particularly, to an adaptive method of compressing computer data files. The file compressor according to the invention is intended for use on files containing language, either computer programs or natural language. The problem of designing a compressor may be separated into two parts. The first is the construction of a model of the source document. For any such model, there is a theoretical minimum size for the compressed document, the so called entropy limit. It is then necessary to formulate a practical coding scheme which will give actual compression close to the theoretical value. For non adaptive models, i.e. models whose characteristics are independent of the document being transmitted, Huffman coding will often be quite efficient. For adaptive models, the coding scheme must also change during the coding process. Adaptive Huffman coding is then required. IEEE transactions on communications, vol. COM 32, no.4, April 1984, pages 396 402, adaptive coding using a Merkov model of the source is described. While such schemes have been built and used, a faster method of coding is desirable. It is therefore an object of the present invention to provide a new file compression method which is both fast and achieves a greater degree of compression than known in the prior art. The present invention is defined in the attached claims. According to the invention, an adaptive method of file compression is achieved by noting that written language can be thought of as a stream of alternating words and separators. To begin with, empty dictionaries are created for the words and separators. For each event in the data stream, a determination is made whether the event is a word or a separator. If the event is a word, a determination is made as to whether the word is in the dictionary for words compiled from the previously encountered words or whether thc word is a new word. If the event is a separator, a similar determination is made using the dictionary for separators. If the event is a new word or a new separator, the event is encoded with a predetermined new word or new separator symbol followed by encoding the characters of the word or separator. A count is maintained of all word events and a count is maintained of all separator events as those events are encoded. In addition, individual counts for each occurrence of a word and each occurrence of a separator are maintained. The ratio of the number of times a word has been encountered to the total number of words encountered is used to estimate the probability of the word. Similarly, the ratio of the number of times a separator has been encountered to the total number of separators encountered is used to estimate the probability of the separator. These probabilities are used with an efficient coding scheme to code the words and separators in the two dictionaries. Arithmetic coding is used in the practical implementation of the file compressor according to the invention. The invention will be better understood from the following detailed description of the preferred embodiment with reference to the drawings, in which The model used in the practice of the invention has the following characteristics. It is assumed that the data to be transmitted consists of a sequence of events, the events being coded one at a time, starting with the first event, then the second, and so forth. When it is time to code the n th event, the model produces 1 the relevant event set and 2 the probability of each event. In doing so, the model may make use of its knowledge of all previous events, but may not take into account the specific n th event or any as yet not encoded event. This restriction means that a copy of the model in the decoder can have the same information as that in the encoder. The model is adaptive i.e., event frequencies are accumulated as the file is encoded. The probabilities are rational fractions, and the probability of a given event is the ratio of its count to the total count for the relevant event set. Thus, the probabilities used change during the coding process. A feature of the model is its selection of the appropriate event set to be used at any step in the coding process. The model may be thought of as a finite state machine. In the dictionary model, a file is treated as a sequence of records. Each record contains an alternating sequence of words and separators , where a word is a string of alphanumeric characters and a separator is a string of non alphanumeric characters. In many documents, the single space will be the dominant separator. The coding process is diagrammed in Figure 1. A record is read from the input file. The record will start with a separator or a word. Thus, the first event set consists of the two possibilities startword or startsep. The appropriate event of the pair is coded. The count of the encountered event is increased by one. When a word is expected, the possible event is one of the following The old word event is one in which the word to be coded has previously been encountered and stored in the dictionary. N is the accession number. The new word event requires further action. The word must be entered in the dictionary, with a count of one, and must also be coded. The coding process for the word is shown in Figure 2. Figure 2 also applies to separators, so the generic term symbol is used. The symbol is coded one character at a time. The event set is as follows The program switches between words and separators until the end of the input record is reached. The end of record event is coded and a new record is read. The coding process is repeated, beginning with the code to indicate whether the record starts with a word or a separator. At the end of the file, the startword and end of record events are transmitted. This denotes an empty record, which is used as the end of a file. The practical implementation of the invention uses arithmetic coding. The arithmetic coding process is first described as if arithmetic may be carried out with unlimited precision. The details of a practical algorithm are given later. To code an event, the input data required are the counts of each of the possible events, in some arbitrary order known to both the compressor and the decompressor , and the particular event to be coded. The variable n denotes the event number. n starts at one and is increased by one as each event is coded. Let c i,n , where i 1, 2, ..., I n , be the set of counts for the I n possible choices for the n th event. Define the cumulative counts as follows Let x n and r n be a pair of real numbers which describes the result of encoding the first n events. The starting values are x and r 1. The coding formulae are, to code the n th event event i in the set described by C are as follows The formulae may become more transparent if the estimated probability and cumulative distribution functions are defined as follows If the total file is described by coding N events, the document is represented by the value of x N . The decoder is given x N and can, using the same model as the encoder, deduce the sequence of N events. The possibility of this decoding follows from the following Since C is a monotonic increasing function of i, only one value of i can satisfy this inequality. The coded file is represented by the value of x N . The number of bits required to represent x N with sufficient accuracy is readily estimated. Any point in the interval X defined by bits will accomplish this. An examination of the formula for calculation of r shows that r is the product of the probabilities of the individual events, so that two more bits are required than the sum for all events of The algorithm, as described, has five event sets and five sets of counts to be maintained. These are given in the tables below For an event in any of the above sets, the event will be coded using the count for the event, the sum of the counts for all events in the set above the event being coded, and the total number of events in the set. After the event is coded, the count for the event is increased by one. If the event is a new word, separator, or character, the count for the new item is set to one, it being previously zero. Note that in this case the total number of events in the set has been increased by two, each new item being a double event in the set. In set 4, the single space blank is treated separately. This is for speed of execution, since in many documents the single space is the predominant separator. The character counts in sets 3 and 5 are indexed by the corresponding EBCDIC code for the character. Since the word characters will not include those with codes of zero or one, these are used for end of word and new character, respectively. For the separator characters, any alphanumeric characters may be used to designate end of separator and new character. For example, the characters x and y may be choosen. These tables of counts except for set 1 are fairly large. The maximum number of items in each set is tabulated below The sizes of the dictionaries sets 2 and 4 are arbitrary. The above choice seems reasonable when compressing actual files for sets 3 and 5, it is convenient to allow 256 entries in each of the tables, although many will not be used. This is for speed of computation. Updating a count for an event is a simple computation. A problem arises when the cumulative count is required. The cumulative counts are defined in equation 1 . A full dictionary would require up to 4 96 additions to obtain a cumulative count. Maintaining the C i,n for each new event would also be time consuming. A technique which solves this problem is available. The approach is indicated in Figure 4 which is constructed for an event set of eight events. The notation is that of equation 1 . Note that the depth of the tree is much smaller than the number of leaves of the tree. Thus, the updating operation requires many less steps. In practice, the tree is traversed once from the top to the desired leaf or event. First, a variable cumulative count is set to zero. During the traversal, if one goes to the left, the count at that node is increased by one. On the other hand, if one goes to the right, the value of the node is added to the cumulative count. When the bottom of the tree is reached, the leaf has the desired value of c i,n and the cumulative count has the value for C i 1,n . For large files, renormalization of the counts may be necessary, either to prevent the counts from getting too large or to keep the dictionary within bounds. Renormalization also causes a deemphasis of old statistics. The renormalization procedure is as follows. If, before an event is processed, the total count for that event set is greater than 16, or the event is a word or a separator and the dictionary for that event set is almost full, renormalization occurs. Each count is divided by two. For words and separators, the remainder is dropped. For other counts, rounding up occurs. For words and separators, some of the entries will go from a count of one to a count of zero. These items are pruned from the dictionary. If such an item is encountered again, it is a new word or a new separator. The coding scheme described above is implemented for a machine with 32 bit words. The details of significance and truncation must be attended to in such a way that the decoding process can go through without error, recovering the original file. All the calculations are carried out in terms of integers. The quantities x and r are both integers. The initial values are as follows According to equation 2 , r gets smaller and smaller. To maintain precision, whenever r gets too small, it is multiplied by 256. The value of x must also be multiplied by 256. This leads to the following scaling operation, which is the first step in the coding process The algorithm will fail if r ever becomes zero. From equations 8 , it is clear that this occurs only if We are safe if Since r is scaled before the coding, the minimum value r can have is 2²³, so all we need is that C I n ,n 2²³. In the proof of inequality 12 given hereinafter which is used for decoding, the condition The decoding process parallels the encoding process. The value of x is obtained from the compressed data. The first four bytes of the compressed data give the initial value of x. The initial value of r is that used in encoding. The statistics for the various event sets are maintained for the decoder as they were for the encoder. Thus, whenever an event is to be decoded, the appropriate event set is known with the correct values of the C s. Renormalization and scaling of x and r are carried out as in encoding. When decoding, the value of x is decreased as much as possible, but not permitted to become less than zero. Thus, the basic decoding formula is based on determining i such that This i specifies the event to be selected from the event set. The new values of r and x are calculated from the following The determination of i from inequality 9 is computationally undesirable, since the cumulative counts and multiple tests are needed. The tree used for managing the statistics is also used in the decoder. We attempt to find a count c such that We first show that From inequality 9 , we obtain so that Since c and C are integers and d is less than one, this yields From inequality 9 , we have so that Since c and C i,n are integers and the first term on the right is less than one, we may conclude that In this case we reduce the trial value of c by one. As a result, we have a value of c which satisfies inequality 11 . This value of c is used in traversing the count tree to find the event that was encoded and the associated counts needed for the calculations in equations 1 .