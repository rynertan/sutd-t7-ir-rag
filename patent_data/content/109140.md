# Recognition of continuous speech.

## Abstract
A speech recognition system extracts feature parame ters from a speech waveform, determines phonemic fea tures using the parameters and encoding segments of the speech waveform, determines the starting and terminal ends of a syllable part from a time series of the encoded segments, extracts the syllable part based on the deter mined starting and terminal ends and then generating a pat tern of the syllable part, and recognizes an input speech through the recognition of each syllable by way of pattern matching of the generated pattern.

## Claims
CLAIMS 1. A speech recognition system comprising means for extracting feature parameters from a speech waveform means for determining phonemic features using said parameters and encoding segments of the speech waveform means for determining the starting and terminal ends of a syllable part from a time series of the encoded segments means for extracting the syllable part based on the determined starting and terminal ends and then generating a pattern of the syllable part and means for recognizing an input speech through the recognition of each syllable by way of pattern matching of said generated pattern. 2. A speech recognition method comprising analysing a spoken word by classifying successive segments of the word according to phonemic features, determining in response to this classification the respective portions of the spoken word which correspond to individual syllables, and comparing the pattern of syllables forming the spoken word with stored patterns in order to recognize the word.

## Description
TITLE OF THE INVENTION RECOGNITION OF CONTINUOUS SPEECH BACKGROUND OF THE INVENTION The present invention relates to an improvement in a speech recognition system for extracting and recognizing syllable parts from continuously produced speech. Known syllable speech recognition systems mainly extracts syllable parts only from simple information such as power levels for pattern matching. Since the speech recognition performance is largely affected by the accuracy of the extraction of syllable parts, no high speech recognition performance can be expected with the simple syllable part extraction method. The simple extraction process is unable to extract syllable parts correctly from the information with external noise overlapped. When syllables are produced at a certain speed or higher three syllables per second or higher, for example , the syllables are liable to be indiscrete and continuous. SUMMARY OF THE INVENTION It is an object of the present invention to provide a speech recognition system which will eliminate the foregoing difficulties by classifying segments hereinafter referred to as frames of speech having a certain time interval in the range of from units of milliseconds to tens of milliseconds according to phonemic features such as voiced and voiceless sounds and nasals and extracting syllable parts using the classified frames. According to the present invention, a speech recognition system extracts feature parameters from a speech waveform, determines phonemic features using the parameters and encoding segments of the speech waveform, determines the starting and terminal ends of a syllable part from a time series of the encoded segments, extracts the syllable part based on the determined starting and terminal ends and then generating a pattern of the syllable part, and recognizes an input speech through the recognition of each syllable by way of pattern matching of the generated pattern. Other objects and further scope of applicability of the present invention will become apparent from the detailed description given hereinafter. It should be understood, however, that the detailed description and specific examples, while indicating preferred embodiments of the invention, are given by way of illustration only, since various changes and modifications within the spirit and scope of the invention will become apparent to those skilled in the art from this detailed description. BRIEF DESCRIPTION OF THE DRAWINGS The present invention will be better understood from the detailed description given hereinbelow and the accompanying drawings which are given by way of illustration only, and thus are not limitative of the present inventin and wherein FIG. 1 is a diagram showing speech sounds expressed as a series of phonemically classified symbols FIG. 2 is a block diagram of a speech recognition system according to the present invention FIG. 3 is a flowchart illustrative of operations of syllable part extraction according to the present invention and FIG. 4 is a diagram showing a series of phonemically classified symbols for utterance DENYAKUKI the English equivalent of which is an electronic translator . DESCRIPTION OF THE PREFERRED EMBODIMENT Prior to describing an embodiment of the present invention, the fundamental principles of speech recognition according to the invention will first be described. According to the embodiment of the invention, segments hereinafter referred to as frames of speech having a certain time interval in the range of from units of milliseconds to tens of milliseconds are classified according to phonemic features such as voiced and voiceless sounds and nasals , and syllable parts are extracted using the classified frames. More specifically, each speech frame is first classified according to phonemic features so as to be expressed by symbols such as , B, N, M, for example , a process hereinafter referred to as phonemic classification . The phonemic properties as expressed by the symbols are set forth in the following table 1 Table 1EMI4.1 tb Symbol SEP Property tb SEP Voiceless SEP part tb SEP B SEP Buzz bar SEP part tb SEP N SEP Nasal SEP part tb SEP M SEP Sound SEP between SEP nasal SEP and SEP vowel SEP and SEP vowel tb SEP part SEP of SEP small SEP power tb SEP V SEP Vowel SEP part tb SEP F SEP Fricatiye SEP part tb SEP C SEP Weak SEP voiceless SEP part. tb FIG. 1 is illustrative of a series of phonemically classified symbols expressing an utterance hanaga . There are four frames of voiceless part in front of he . Since this voiceless part is followed by four unvoiced frames, the voiceless part is regarded as noise and excluded from the extraction of the voiced sound. The consonant part in the front half of ha is composed of fricative frames, and there is one unvoiced frame in a transient zone to the vowel part. The front fricative part and the rear vowel part, including the one voiceless zone, are regarded wholley as a syllable part. Generally, a few unvoiced frames appear in many instances after the explosive part of a voiceless stop. In such an instance, it is necessary to extract the plosive part including the unvoiced part and the vowel part as a syllable part. The syllable part na is extracted from the first nasal symbol N through the vowl part. A vocal cord consisting only of low frequency components known as a buzz bar part is frequently detected prior to the explosive part of a voiced explosive. Such a vocal cord is identified by successive symbols B and the frames of the symbols B are not handled as a voiced period. The syllable part ga is extracted by excluding the frames B prior thereto. For thus extracting the syllable parts from the phonemically classified symbols, various processes are carried out such as the processing of voiceless periods, the processing of noise part, the division of vowel and consonant parts in boundaries between the syllables. In addition to the above processing, it is necessary to smooth symbpl levels as the phonemically classified symbols tend to appear unstably dependent on the feature parameters used in syllable classification and on the manner of syllable classification. As the speed of utterance is increased, there are sometimes occasions in which syllable parts cannot uniquely be extracted only with the series of phonemically classified symbols. When this happens, it is necessary to use additional information such for example as time dependent changes in spectrum and recesses hereinafter referred to as power dips in time dependent power patterns. FIG. 2 shows in block form a speech recognition system according to the present invention. As shown in FIG. 2, an utterance picked up by a microphone not shown is fed through an amplifier and a preemphasis circuit both not shown , sampled and then converted into a digital signal through A D conversion.The quantitized digital signal is delivered through a terminal 1 into a speech analyzer 2. The speech analyzer 2 determines at constant time intervals in the range of from few units of milliseconds to tens of milliseconds parameters for obtaining general features of the utterance and parameters forming syllable patterns such as parameters generated by a linear predictive analysis and parameters representative of a spectrum . The parameters forming syllable patterns are stored in a pattern memory 3. Based on the foregoing parameters, a phonemic classifier 5 effects phonemic classification at constant time intervals and issues the foregoing series of phonemically classified symbols. A syllable extractor 6 is responsive to the symbol seires for detecting the starting and terminal ends of the syllables as described with reference to FIG. 1, and issues the addresses in the pattern memory 3 to a pattern generator 4. The pattern generator 4 serves to generate a pattern by compressing or smoothing out on a time basis the syllabic pattern selected in the pattern memory 3. At the time of speech registration through the inputting of standard speech, the patterns are stored in a standard pattern memory 7 through the route illustrated by the broken line in FIG. 2. When an utterance to be recognized is entered, a pattern generated by the pattern generator 4 is delivered to a matching unit 8 which computes the distance and liklihood of the pattern from the pattern generator 4 with respect to each standard pattern stored in the standard pattern memory 7. One example of such a distance is the euclidean distanceEMI7.1 The pattern is composed of data of I frames and comprises an IJ dimensional vector constituted of J frames. Xij denotes an unknown pattern entered, and Yij denotes one of the standard patterns, Xij and Yij being normalized with respect to time in the J frames. Other distances subjected to various processes such as the one using DP matching may also be employed. A discriminator 9 determines which standard pattern is most approximated by the input unknown pattern based on the foregoing values of distance and likihood, and delivers the result of discrimination to an output unit 10. There are normally a plurality of standard patterns present for a certain syllable As a measure of similarity for a syllable, the average distance and likelihood with respect to the plural standard patterns may be used, or alternatively the distance and likelihood with respect to the standard pattern to which the entered pattern is most similar may be employed. Where spectra changes and power dips are both used as additional information, these quantities should be computed in the speech analyzer 2 or the syllable extractor 6 while storing the parameters of a frame prior to or frames prior and subsequent to a frame under consideration, since the quantities are composed of information between frames. The quantities indicative of spectrum variations may comprise, for example, a cepstrum coefficient, a parameter obtained by the linear predictive analysis, and a timedependent change in the output from a filter bank. The power dip may be indicated by the presence or absence of the minimal value in the time dependnt change of the power of a processed waveform such as a power or difference in a certain frequency band. The spectrum variation or power dip information can be utilized for example for detecting the starting end of a syllable composed of a number of successive voiced frames the phonemically classified symbols N, M, V with no starting end detected prior to the successive voiced frames. A method of syllable extraction according to the present invention will be described. FIG. 3 is a flowchart illustrative of such a syllable extraction method. For the brevity of illustration, the phonemically classified symbols set forth in the table 1 are grouped into four symbols , C, N, V. The symbol is representative of voiceless parts including the symbols and B the symbol C of fricative parts including the symbols C and F, the symbol N of nasal parts, and the symbol V of vowel parts including M and V. The following table 2 shows the above series of phonemically classified symbols. Table 2EMI9.1 tb Zone SEP Series SEP of SEP phonemically SEP classified SEP symbols tb a SEP F SEP SEP E3 SEP ................ SEP tb b SEP . SEP CQ.. SEP V SEP V SEP V SEP V SEP V SEP V tb c SEP . SEP 4 SEP C SEP C SEP C SEP C SEP C SEP C26 SEP V SEP V SEP V SEP V SEP V SEP V tb d SEP ........ SEP C SEP V SEP V SEP V SEP V SEP V SEP V SEP V tb e SEP .... SEP V SEP V SEP V SEP V SEP N SEP N SEP N SEP V SEP V SEP V SEP V tb f SEP V SEP V SEP V SEP V SEP V SEP V SEP V SEP V SEP C SEP C SEP C SEP C SEP C SEP V SEP V SEP V tb g SEP V SEP V SEP V SEP V SEP V SEP V SEP V SEP V SEP ........ SEP tb h SEP VVVVVNNNCCCCCCVV SEP tb The method of syllable extraction will be described with reference to the example given above. A step n2 in FIG. 3 detects whether a frame under consideration the frames enclosed in rectangle in the Table 2 and a previous frame are indicated by . If the frame is in the voiceless zone, no successive processes are carried out and the program goes back to a step nl in which the frame under consideration is renewed and a next frame will be processed. A step n3 detects whether the frame under consideration is a voiceless part few frames subsequent to a plosive part at b in the Table 2, a plosive part detected is indicated by the sign above the symbol or a fricative part at c in the Table 2 .If the frame is a transient voiceless part, then no successive process is performed and the program returns to the step n1 in which the frame under consideration is renewed and a next frame will be processes. Since no counter renewal described later is effected at this time, the feature vector of the transient voiceless part will not be contained in the pattern of the syllable. Thus, a pattern is generated with the transient voiceless part neglected. A step n4 detects whether the phonemically classified symbol has changed on a time basis. For example, the Table 2 indicates at d a change from a voiceless zone to a voiced zone considered to be the starting end of a syllable across the frame now under consideration. If the symbol change is detected, then the program goes to a step n8 in which a syllable end is detected. For e , f , g in the Table 2, the symbols are also regarded as having varied, and the program proceeds to the step n8. A step n5 detects whether the frame under consideration is subjected to a minimum power level as compared with frames immediately prior and subsequent to the frame under consideration. The power dips include those varying gradually in time and those varying quickly in time, and various power dips can therefore be employed also dependent on which power is used. Powers used include the power of a particular frequency band, the power of a pre emphasized waveform with high or low frequency ranges emphasized , and others. Such various powers can also be used in combination for discrimination purpose. The power dips include deep and shallow ones. Therefore, when a power dip exceeds a certain threshold, the step n8 uses the depth of the power dip, information as to how steep the power dip is, and information indicating which power is used for detecting an end of a syllable. A next step n6 detects spectrum changes by using various data such as information as to how quick the spectrum changes, how the spectrum is intense, a physical quantities representative of a spectrum change, and combinations of such data. The physical quantities indicating a spectrum change include time dependent changes such as an autocorrelation coefficient, a cepstrum coefficient, and parameters obtained in the linear predicative analysis such as an parameter, a k parameter adn the like , and time depedent variations in the filter bank output.These changes may be given as the distance between frames spaced by a time interval ranging from over ten milliseconds to tens of milliseconds according to the following formula EMI12.1 where the parameter Cij is the cepstrum coefficient, i the degree, j the frame number, I the maximum degree of the cepstrum coefficient, J the frame number for computing the change, and k the time interval for finding the change.Thus, the formula indicates the euclidean distance of the pattern spaced between the J frames by the k frames. The syllable starting end detection effected by the step n8 divides the utterance into the following five states based on hte above phonemically divided symbol series, the power dip information, and the spectrum change information . l Starting end portion see d in the Table 2 , 2 Transient portion see e in the Table 2 , 3 Boundary between syllables see f in the Table 2 , 4 Terminal end portion see g in the Table 2 , and 5 Removal of transient portion at syllable boundary see h in the Table 2 . The example given in the Table 2 however illustrates changes in the phonemically classified symbols only.In the status 5 , the frames underscored have already been extracted as a syllable part and completed pattern matching, and those underscored with a thicker line appear as a transient portion When counters described later are reset under this condition, the frames underscored with the thicker line are excluded see h in the Table 2 . The counter serves to count the symbols , C, N, V and the frames from the starting end of the syllable. In the status 2 see e in the Table 2 , a symbol change between voiced frames is detected in a few frames subsequent to an apparent syllabic starting end characterized by a change from a voiceless zone to a voiced zone. In this instance, the frame now under consideration cannot be regarded as a syllabic starting end, and the processing goes on to following frames. At this time, the counters for the voiced sounds N, V are referred to in a step n7. For the example c in the Table 2, only the counter for the symbol C and the counter for the frame number have their counts,.and the unvoiced frame now under consideration can be regarded as a transient voiceless part in a syllable.In this manner, judgment can be made while referring to the contents of the counters in each detecting step shown in FIG. 3. There are instances wherein the series of phonemically classified symbols is arranged irregularly, and a syllable boundary is present in a series of the same symbols. The syllabic starting end detecting step detects whether it is a syllable boundary by referring to the counter contents while using information such as a power dip and a spectrum variation. If the frame number exceeds a certain threshold with no symbol change, no power dip, and no spectrum change being detected, the program can go to the step n8 for the detection of a syllabic starting end by referring to the counter contents. The phonemic classifier 5 is capable of classifying the frames not only with the general features of a speed spectrum, but also with more specific information. For example, such information may be used which has been subjected to pattern matching for a feature vector composed of parameters used as feature parameters of each frame for forming syllable patterns. Where cepstrum coefficients up to 24th are used as parameters, the feature vector becomes a 24th vector which can express the features of the frames. Featrue vectors corresponding to vowel parts in syllable patterns obtained in advance by speech for registration are collected, and standard feature vectors of respective vowels a , i , u , e , o are generated.These standard feature vectors will hereinafter be called phoneme standard patterns . The phoneme standard patterns may be generated in various ways. For example, a phoneme standard pattern for a can be produced by collecting and averaging the feature vectors of vowel parts of syllable patterns having subsequent vowels a , or few phoneme standard patterns for a can be prepared by clustering. A series of phoneme symbols can thus be obtained by matching phoneme standard patterns previously prepared with feature vectors of respective frames of input speech to determine which phoneme standard pattern is most approximated by the feature vector of each frame. Therefore, more detailed information can be obtained by utilizing the phoneme symbol series and the matching distance with respect to the phoneme standard patterns. There can be prepared phoneme standard patterns corresponding not only to five vowels but also to other phonemes. For instance, phoneme standard patterns for nasals and fricatives can be prepared by collecting the feature vectors of consonant parts of nasals and fricatives marked with the nasal symbol N and fricative symbol F see the Table 1 according to the phonemic classification, thus generating consonant phoneme standard patterns similar to vowel phoneme standard patterns. It is also possible to collect phoneme standard patterns from a multiplicity of speakers and prepare few phoneme standard patterns for each phoneme by way of clustering. By using these phoneme standard patterns, it is rendered possible to mark frames with symbols for unspecified speakers. Since the phonemically classified symbols see the Table 1 are independent of speakers, symbol marking applicable to unspecified speakers can be made through combination of matching with phoneme standard patterns for unspecified speakers. Phoneme standard patterns can also be prepared for some groups of speakers for example, male speakers, female speakers, children speakers, aged speakers, and the like. The phonemic classifier can thus extract detailed phonemic features for marking them with symbols by using not only phonemically classified symbols but also using other information. The phonemic classifier effects symbol marking for each frame, but can be improved by employing information prior and subsequent to the frame under consideration.This can not only perform smoothing, but also utilize time dependent change features. For example, when aia is uttered slowly, a series of phoneme symbols appear like AAAAAAAAEEEEIII II IEEEEAAAAAAAAAAAA and when the same is uttered more quickly, the phoneme symbol series AAAAAAAAAAAEEEEEEEEEAAAAAAAAAAA appears. The distance to the phoneme standard pattern of i may be minimum at the postion of EEEE ..., and this can be utilized to rewrite the symbol E as the symbol I for correcting the series into a series marked with simbols corresponding to the utterance which is acutually heard. In certain instances, it is better for the program to proceed from the symbol transient detection in the step n4 of FIG. 3 to the syllabic starting end detecting step when a suitable post ion as a syllabic starting end is assumed, rather than going to the syllabic starting end detecting step only upon detection of a symbol transition. For exmaple, when iya is uttered, the vowel phoneme symbol series appears in many occasions as IIIIIIIIIIIIIIIEEEEAAAAAAAAAAAAA. The recognition performance is sometimes greater when the frame marked with is regarded as a starting end than when a point of change from I to E is regarded as a starting end. Consequently, the program can go to the syllabic starting end detecting step from that frame with the symbol transistion being regarded as detected. Where the more detailed symbol series is employed, therefore, the symbol transistion detection not only contains symbol change detection, but also has a function including syllable boundary detection, for improved performance. In the foregoing embodiment, the starting and terminal ends of a syllable part are determined uniquely.In actual continuous speech, however, it is frequent that candidate times for the starting and terminal ends cannot be defined solely in a certain interval of time. FIG. 4 shows an example df a series of symbols obtained when DENYAKUKI meaning electronic translator is uttered, the view showing a portion of such symbols. Rectangles below the symbol series are representative of an instance in which candidate zones for syllables are determined without using information on spectrum changes and power dips. Assuming that there are obtained candidate zones as indicated by numbers in the rectangles between N and A, 2, 3, 4 , 2, 3, 8 , 2, 6 , 2, 7 , 2, 8 , 5, 4 , 5, 6 , and the like are attained as candidates for syllable zones on the series.Let the results first candidate of pattern matching between the patterns for the zone candidates from 1 to 9 be de l , n 2 , i 3 , a 4 , ni 5 , ya 6 , ya 7 , ya 8 , ku 9 the numbers in the parentheses are the nubmers of the illustrated syllable zone candidates . If ski 10 not shown is obtained, syllable series composed of first candidates are DENIAKUKI, DENIYAKUKI, DENYAKUKI, DENIAKUKI, and DENIYAKUKI. With the present system employed for word recognition and only denyakuki is contained as the above syllable series among the words registered in the word dictionary the other alternative syllable series are not in general use , the result of recognition becomes DENYAKUKI . With the present invention, as described above, feature parameters are extracted from a speech waveform, phonemic features of an utterance are determined from the parameters to encode segments of the speed waveform, and the starting and terminal ends of a syllable part are determined from the time series of the encoded segments, thereby extracting the syllable. Therefore, the syllable part can accurately be extracted for thereby improving the performance of recognition of continuously uttered sounds. The invention thus being described, it will be obvious that the same may be varied in many ways. Such variations are not to be regarded as a departure from the spirit and scope of the invention. There are described above novel features which the skilled man will appreciate give ri to advantages. These are each independent aspects of the invention to be covered by the present application, irrespective of whether they are included within the scope of the following claims.